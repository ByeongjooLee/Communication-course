{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¬ ìœ íŠœë¸Œ ëŒ“ê¸€ ë¶„ì„: YouTube API + KoBERT ê°ì„±ë¶„ì„ + í˜ì˜¤í‘œí˜„ íƒì§€\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "1. **YouTube Data API**ë¥¼ í™œìš©í•˜ì—¬ ê³µì‹ì ìœ¼ë¡œ ëŒ“ê¸€ì„ ìˆ˜ì§‘í•œë‹¤\n",
    "2. **KoBERT ê¸°ë°˜ ë”¥ëŸ¬ë‹ ê°ì„±ë¶„ì„**ì„ ìˆ˜í–‰í•œë‹¤\n",
    "3. í˜ì˜¤í‘œí˜„ì„ íƒì§€í•˜ê³  ë¶„ë¥˜í•œë‹¤\n",
    "4. ë¶„ì„ ê²°ê³¼ë¥¼ ì‹œê°í™”í•œë‹¤\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š ëª©ì°¨\n",
    "1. [í™˜ê²½ ì„¤ì •](#1-í™˜ê²½-ì„¤ì •)\n",
    "2. [YouTube API ì„¤ì •](#2-youtube-api-ì„¤ì •)\n",
    "3. [ë™ì˜ìƒ ê²€ìƒ‰ ë° ëŒ“ê¸€ ìˆ˜ì§‘](#3-ë™ì˜ìƒ-ê²€ìƒ‰-ë°-ëŒ“ê¸€-ìˆ˜ì§‘)\n",
    "4. [í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬](#4-í…ìŠ¤íŠ¸-ì „ì²˜ë¦¬)\n",
    "5. [KoBERT ê°ì„±ë¶„ì„](#5-kobert-ê°ì„±ë¶„ì„)\n",
    "6. [í˜ì˜¤í‘œí˜„ íƒì§€](#6-í˜ì˜¤í‘œí˜„-íƒì§€)\n",
    "7. [ì¢…í•© ë¶„ì„ ë° ì‹œê°í™”](#7-ì¢…í•©-ë¶„ì„-ë°-ì‹œê°í™”)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”‘ YouTube Data APIë€?\n",
    "- Googleì—ì„œ ì œê³µí•˜ëŠ” ê³µì‹ YouTube API\n",
    "- ë™ì˜ìƒ ê²€ìƒ‰, ì±„ë„ ì •ë³´, ëŒ“ê¸€ ìˆ˜ì§‘ ë“± ê°€ëŠ¥\n",
    "- **ì¼ì¼ í• ë‹¹ëŸ‰**: 10,000 units (ëŒ“ê¸€ 1íšŒ ìš”ì²­ â‰ˆ 1 unit)\n",
    "- ë¬´ë£Œë¡œ ì‚¬ìš© ê°€ëŠ¥ (Google Cloud Consoleì—ì„œ API í‚¤ ë°œê¸‰)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 1-1. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
    "# ============================================\n",
    "\n",
    "# YouTube API í´ë¼ì´ì–¸íŠ¸\n",
    "!pip install google-api-python-client -q\n",
    "\n",
    "# í•œêµ­ì–´ ìì—°ì–´ì²˜ë¦¬\n",
    "!pip install konlpy -q\n",
    "\n",
    "# ë°ì´í„° ë¶„ì„ ë° ì‹œê°í™”\n",
    "!pip install pandas numpy matplotlib seaborn wordcloud -q\n",
    "\n",
    "# í† í”½ ëª¨ë¸ë§\n",
    "!pip install gensim -q\n",
    "\n",
    "# KoBERT ë”¥ëŸ¬ë‹\n",
    "!pip install transformers torch -q\n",
    "!pip install sentencepiece -q\n",
    "\n",
    "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 1-2. Colab í™˜ê²½ ì„¤ì • (Java, í•œê¸€ í°íŠ¸)\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "def setup_colab_environment():\n",
    "    \"\"\"Colab í™˜ê²½ ì„¤ì •: Java + í•œê¸€ í°íŠ¸\"\"\"\n",
    "    \n",
    "    print(\"ğŸ”„ Java ì„¤ì¹˜ ì¤‘...\")\n",
    "    os.system('apt-get update -qq')\n",
    "    os.system('apt-get install -y openjdk-11-jdk-headless -qq')\n",
    "    os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "    \n",
    "    print(\"ğŸ”„ í•œê¸€ í°íŠ¸ ì„¤ì¹˜ ì¤‘...\")\n",
    "    os.system('apt-get install -y fonts-nanum -qq')\n",
    "    fm._load_fontmanager()\n",
    "    \n",
    "    plt.rcParams['font.family'] = 'NanumBarunGothic'\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    \n",
    "    print(\"âœ… í™˜ê²½ ì„¤ì • ì™„ë£Œ!\")\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    setup_colab_environment()\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    print(\"ë¡œì»¬ í™˜ê²½ì…ë‹ˆë‹¤.\")\n",
    "    plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "    IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 1-3. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "# ============================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# YouTube API\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "\n",
    "# í•œêµ­ì–´ NLP\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# ë”¥ëŸ¬ë‹\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "print(\"âœ… ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ!\")\n",
    "print(f\"ğŸ“… ì‹¤í–‰ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"ğŸ”§ PyTorch: {torch.__version__}\")\n",
    "print(f\"ğŸ”§ GPU ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. YouTube API ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 2-1. API í‚¤ ë° ê²€ìƒ‰ ì„¤ì •\n",
    "# ============================================\n",
    "\n",
    "# â¬‡ï¸â¬‡ï¸â¬‡ï¸ ì—¬ê¸°ë¥¼ ìˆ˜ì •í•˜ì„¸ìš”! â¬‡ï¸â¬‡ï¸â¬‡ï¸\n",
    "\n",
    "# YouTube Data API í‚¤\n",
    "API_KEY = \"AIzaSyBFsjYPVWNQml-4vLG9zeSV8mwVM(ì˜ˆì‹œì„)\"\n",
    "\n",
    "# ê²€ìƒ‰ í‚¤ì›Œë“œ\n",
    "KEYWORD = \"ì´íƒœì›ì°¸ì‚¬\"\n",
    "\n",
    "# ìˆ˜ì§‘ ì„¤ì •\n",
    "MAX_VIDEOS = 10          # ê²€ìƒ‰í•  ìµœëŒ€ ë™ì˜ìƒ ìˆ˜\n",
    "MAX_COMMENTS = 100       # ë™ì˜ìƒë‹¹ ìµœëŒ€ ëŒ“ê¸€ ìˆ˜\n",
    "\n",
    "# â¬†ï¸â¬†ï¸â¬†ï¸ ì—¬ê¸°ë¥¼ ìˆ˜ì •í•˜ì„¸ìš”! â¬†ï¸â¬†ï¸â¬†ï¸\n",
    "\n",
    "print(f\"ğŸ”‘ API í‚¤: {API_KEY[:20]}...\")\n",
    "print(f\"ğŸ” ê²€ìƒ‰ í‚¤ì›Œë“œ: {KEYWORD}\")\n",
    "print(f\"ğŸ¬ ìµœëŒ€ ë™ì˜ìƒ ìˆ˜: {MAX_VIDEOS}ê°œ\")\n",
    "print(f\"ğŸ’¬ ë™ì˜ìƒë‹¹ ìµœëŒ€ ëŒ“ê¸€: {MAX_COMMENTS}ê°œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 2-2. YouTube API í´ë¼ì´ì–¸íŠ¸ ìƒì„±\n",
    "# ============================================\n",
    "\n",
    "def create_youtube_client(api_key):\n",
    "    \"\"\"\n",
    "    YouTube Data API v3 í´ë¼ì´ì–¸íŠ¸ ìƒì„±\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    api_key : str\n",
    "        Google API í‚¤\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    googleapiclient.discovery.Resource\n",
    "    \"\"\"\n",
    "    try:\n",
    "        youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "        print(\"âœ… YouTube API í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì™„ë£Œ!\")\n",
    "        return youtube\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ API í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}\")\n",
    "        return None\n",
    "\n",
    "# í´ë¼ì´ì–¸íŠ¸ ìƒì„±\n",
    "youtube = create_youtube_client(API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 2-3. API ì—°ê²° í…ŒìŠ¤íŠ¸\n",
    "# ============================================\n",
    "\n",
    "def test_api_connection(youtube):\n",
    "    \"\"\"API ì—°ê²° í…ŒìŠ¤íŠ¸\"\"\"\n",
    "    try:\n",
    "        request = youtube.search().list(\n",
    "            part='snippet',\n",
    "            q='test',\n",
    "            type='video',\n",
    "            maxResults=1\n",
    "        )\n",
    "        response = request.execute()\n",
    "        print(\"âœ… API ì—°ê²° í…ŒìŠ¤íŠ¸ ì„±ê³µ!\")\n",
    "        return True\n",
    "    except HttpError as e:\n",
    "        print(f\"âŒ API ì˜¤ë¥˜: {e}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì—°ê²° ì‹¤íŒ¨: {e}\")\n",
    "        return False\n",
    "\n",
    "# API í…ŒìŠ¤íŠ¸\n",
    "if youtube:\n",
    "    api_works = test_api_connection(youtube)\n",
    "else:\n",
    "    api_works = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. ë™ì˜ìƒ ê²€ìƒ‰ ë° ëŒ“ê¸€ ìˆ˜ì§‘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 3-1. í‚¤ì›Œë“œë¡œ ë™ì˜ìƒ ê²€ìƒ‰\n",
    "# ============================================\n",
    "\n",
    "def search_videos(youtube, keyword, max_results=10):\n",
    "    \"\"\"\n",
    "    í‚¤ì›Œë“œë¡œ YouTube ë™ì˜ìƒ ê²€ìƒ‰\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    youtube : googleapiclient.discovery.Resource\n",
    "        YouTube API í´ë¼ì´ì–¸íŠ¸\n",
    "    keyword : str\n",
    "        ê²€ìƒ‰ í‚¤ì›Œë“œ\n",
    "    max_results : int\n",
    "        ìµœëŒ€ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list : ë™ì˜ìƒ ì •ë³´ ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    \n",
    "    videos = []\n",
    "    \n",
    "    print(f\"ğŸ”„ '{keyword}' ë™ì˜ìƒ ê²€ìƒ‰ ì¤‘...\")\n",
    "    \n",
    "    try:\n",
    "        request = youtube.search().list(\n",
    "            part='snippet',\n",
    "            q=keyword,\n",
    "            type='video',\n",
    "            order='relevance',  # ê´€ë ¨ì„±ìˆœ (viewCount: ì¡°íšŒìˆ˜ìˆœ, date: ìµœì‹ ìˆœ)\n",
    "            regionCode='KR',\n",
    "            relevanceLanguage='ko',\n",
    "            maxResults=max_results\n",
    "        )\n",
    "        response = request.execute()\n",
    "        \n",
    "        for item in response.get('items', []):\n",
    "            video_id = item['id']['videoId']\n",
    "            snippet = item['snippet']\n",
    "            \n",
    "            videos.append({\n",
    "                'video_id': video_id,\n",
    "                'title': snippet['title'],\n",
    "                'channel': snippet['channelTitle'],\n",
    "                'published_at': snippet['publishedAt'][:10],\n",
    "                'url': f'https://www.youtube.com/watch?v={video_id}'\n",
    "            })\n",
    "        \n",
    "        print(f\"âœ… {len(videos)}ê°œ ë™ì˜ìƒ ê²€ìƒ‰ ì™„ë£Œ!\")\n",
    "        \n",
    "    except HttpError as e:\n",
    "        print(f\"âŒ API ì˜¤ë¥˜: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {e}\")\n",
    "    \n",
    "    return videos\n",
    "\n",
    "# ë™ì˜ìƒ ê²€ìƒ‰\n",
    "if youtube and api_works:\n",
    "    videos = search_videos(youtube, KEYWORD, MAX_VIDEOS)\n",
    "else:\n",
    "    videos = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 3-2. ê²€ìƒ‰ëœ ë™ì˜ìƒ ëª©ë¡ í™•ì¸\n",
    "# ============================================\n",
    "\n",
    "if videos:\n",
    "    print(f\"\\nğŸ“‹ ê²€ìƒ‰ëœ ë™ì˜ìƒ ëª©ë¡ ({len(videos)}ê°œ):\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for i, video in enumerate(videos, 1):\n",
    "        title = video['title'][:50] + \"...\" if len(video['title']) > 50 else video['title']\n",
    "        print(f\"\\n{i}. {title}\")\n",
    "        print(f\"   ğŸ“º ì±„ë„: {video['channel']}\")\n",
    "        print(f\"   ğŸ“… ê²Œì‹œì¼: {video['published_at']}\")\n",
    "        print(f\"   ğŸ”— {video['url']}\")\n",
    "else:\n",
    "    print(\"âŒ ê²€ìƒ‰ëœ ë™ì˜ìƒì´ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 3-3. ë™ì˜ìƒ ëŒ“ê¸€ ìˆ˜ì§‘ í•¨ìˆ˜\n",
    "# ============================================\n",
    "\n",
    "def get_video_comments(youtube, video_id, max_comments=100):\n",
    "    \"\"\"\n",
    "    ë™ì˜ìƒì˜ ëŒ“ê¸€ ìˆ˜ì§‘ (YouTube API)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    youtube : googleapiclient.discovery.Resource\n",
    "        YouTube API í´ë¼ì´ì–¸íŠ¸\n",
    "    video_id : str\n",
    "        ë™ì˜ìƒ ID\n",
    "    max_comments : int\n",
    "        ìµœëŒ€ ëŒ“ê¸€ ìˆ˜\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list : ëŒ“ê¸€ ë°ì´í„° ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    \n",
    "    comments = []\n",
    "    next_page_token = None\n",
    "    \n",
    "    try:\n",
    "        while len(comments) < max_comments:\n",
    "            request = youtube.commentThreads().list(\n",
    "                part='snippet',\n",
    "                videoId=video_id,\n",
    "                order='relevance',  # ê´€ë ¨ì„±ìˆœ (time: ìµœì‹ ìˆœ)\n",
    "                maxResults=min(100, max_comments - len(comments)),\n",
    "                pageToken=next_page_token,\n",
    "                textFormat='plainText'\n",
    "            )\n",
    "            response = request.execute()\n",
    "            \n",
    "            for item in response.get('items', []):\n",
    "                comment = item['snippet']['topLevelComment']['snippet']\n",
    "                \n",
    "                comments.append({\n",
    "                    'text': comment['textDisplay'],\n",
    "                    'author': comment['authorDisplayName'],\n",
    "                    'likes': comment['likeCount'],\n",
    "                    'published_at': comment['publishedAt'][:10],\n",
    "                    'reply_count': item['snippet']['totalReplyCount']\n",
    "                })\n",
    "            \n",
    "            # ë‹¤ìŒ í˜ì´ì§€\n",
    "            next_page_token = response.get('nextPageToken')\n",
    "            if not next_page_token:\n",
    "                break\n",
    "        \n",
    "    except HttpError as e:\n",
    "        # ëŒ“ê¸€ì´ ë¹„í™œì„±í™”ëœ ë™ì˜ìƒ\n",
    "        if 'commentsDisabled' in str(e):\n",
    "            pass\n",
    "        else:\n",
    "            pass  # ë‹¤ë¥¸ ì˜¤ë¥˜ ë¬´ì‹œ\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 3-4. ëª¨ë“  ë™ì˜ìƒì—ì„œ ëŒ“ê¸€ ìˆ˜ì§‘\n",
    "# ============================================\n",
    "\n",
    "all_comments = []\n",
    "\n",
    "if videos and youtube:\n",
    "    print(f\"ğŸ”„ {len(videos)}ê°œ ë™ì˜ìƒì—ì„œ ëŒ“ê¸€ ìˆ˜ì§‘ ì‹œì‘...\\n\")\n",
    "    \n",
    "    for i, video in enumerate(videos, 1):\n",
    "        title_short = video['title'][:40] + \"...\" if len(video['title']) > 40 else video['title']\n",
    "        print(f\"[{i}/{len(videos)}] {title_short}\")\n",
    "        \n",
    "        comments = get_video_comments(youtube, video['video_id'], MAX_COMMENTS)\n",
    "        \n",
    "        # ë™ì˜ìƒ ì •ë³´ ì¶”ê°€\n",
    "        for comment in comments:\n",
    "            comment['video_id'] = video['video_id']\n",
    "            comment['video_title'] = video['title']\n",
    "            comment['channel'] = video['channel']\n",
    "        \n",
    "        all_comments.extend(comments)\n",
    "        print(f\"      âœ… {len(comments)}ê°œ ëŒ“ê¸€ ìˆ˜ì§‘\")\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"âœ… ì´ {len(all_comments)}ê°œ ëŒ“ê¸€ ìˆ˜ì§‘ ì™„ë£Œ!\")\n",
    "else:\n",
    "    print(\"âš ï¸ ë™ì˜ìƒì´ ì—†ê±°ë‚˜ API ì—°ê²° ì‹¤íŒ¨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 3-5. ë°ì´í„°í”„ë ˆì„ ìƒì„± (ë˜ëŠ” ìƒ˜í”Œ ë°ì´í„°)\n",
    "# ============================================\n",
    "\n",
    "def create_sample_comments():\n",
    "    \"\"\"ìƒ˜í”Œ ëŒ“ê¸€ ë°ì´í„° ìƒì„±\"\"\"\n",
    "    sample_data = [\n",
    "        # ì• ë„/ì¶”ëª¨\n",
    "        {\"text\": \"ìœ ê°€ì¡±ë¶„ë“¤ê»˜ ê¹Šì€ ìœ„ë¡œë¥¼ ì „í•©ë‹ˆë‹¤. ë‹¤ì‹œëŠ” ì´ëŸ° ì¼ì´ ì—†ì–´ì•¼ í•©ë‹ˆë‹¤.\", \"likes\": 1245},\n",
    "        {\"text\": \"í¬ìƒìë¶„ë“¤ì˜ ëª…ë³µì„ ë¹•ë‹ˆë‹¤. ì§„ìƒê·œëª…ì´ ë°˜ë“œì‹œ ì´ë£¨ì–´ì ¸ì•¼ í•©ë‹ˆë‹¤.\", \"likes\": 987},\n",
    "        {\"text\": \"ë„ˆë¬´ ì•ˆíƒ€ê¹ìŠµë‹ˆë‹¤. ì‚¼ê°€ ê³ ì¸ì˜ ëª…ë³µì„ ë¹•ë‹ˆë‹¤.\", \"likes\": 876},\n",
    "        {\"text\": \"ìŠì§€ ì•Šê² ìŠµë‹ˆë‹¤. í¬ìƒìë¶„ë“¤ì„ ê¸°ì–µí•©ë‹ˆë‹¤.\", \"likes\": 543},\n",
    "        {\"text\": \"ìœ ê°€ì¡±ë¶„ë“¤ í˜ë‚´ì„¸ìš”. ì‘ì›í•©ë‹ˆë‹¤.\", \"likes\": 432},\n",
    "        \n",
    "        # ë¹„íŒ/ë¶„ë…¸\n",
    "        {\"text\": \"ì±…ì„ì ì²˜ë²Œì´ ì œëŒ€ë¡œ ì´ë¤„ì ¸ì•¼ í•©ë‹ˆë‹¤. ì´ê²Œ ë‚˜ë¼ì…ë‹ˆê¹Œ?\", \"likes\": 2134},\n",
    "        {\"text\": \"ê²½ì°°ì€ ë­˜ í–ˆë‚˜ìš”? ê·¸ ë§ì€ ì‹ ê³ ë¥¼ ë¬´ì‹œí•˜ë‹¤ë‹ˆ ë§ì´ ë©ë‹ˆê¹Œ?\", \"likes\": 1876},\n",
    "        {\"text\": \"ì •ë¶€ì˜ ë¬´ëŠ¥ì´ ë§Œë“  ì°¸ì‚¬ì…ë‹ˆë‹¤. ì±…ì„ì§€ëŠ” ì‚¬ëŒì´ ì—†ë„¤ìš”.\", \"likes\": 1432},\n",
    "        {\"text\": \"ì•„ì§ë„ ì±…ì„ì§€ëŠ” ì‚¬ëŒì´ ì—†ë‹¤ë‹ˆ í™”ê°€ ë‚©ë‹ˆë‹¤.\", \"likes\": 1234},\n",
    "        {\"text\": \"ë„ˆë¬´ ë¶„ë…¸ê°€ ì¹˜ë°€ì–´ ì˜¤ë¦…ë‹ˆë‹¤. ì™œ ì´ëŸ° ì¼ì´ ë°˜ë³µë˜ë‚˜ìš”?\", \"likes\": 1123},\n",
    "        {\"text\": \"ì•ˆì „ ë¶ˆê°ì¦ì˜ ê²°ê³¼ì…ë‹ˆë‹¤. ì •ë§ ë‹µë‹µí•©ë‹ˆë‹¤.\", \"likes\": 987},\n",
    "        \n",
    "        # ì •ì¹˜ ê³µë°©\n",
    "        {\"text\": \"ë˜ ì •ì¹˜ì ìœ¼ë¡œ ì´ìš©í•˜ë ¤ê³ ? í¬ìƒìë“¤ ë‘ ë²ˆ ì£½ì´ì§€ ë§ˆì„¸ìš”.\", \"likes\": 1567},\n",
    "        {\"text\": \"ì•¼ë‹¹ì€ ë§¨ë‚  ì •ìŸë§Œ... ì§„ì§œ ì¶”ëª¨í•  ìƒê°ì´ë‚˜ ìˆëŠ” ê±´ê°€?\", \"likes\": 1345},\n",
    "        {\"text\": \"ì—¬ë‹¹ì´ë‚˜ ì•¼ë‹¹ì´ë‚˜ ì •ì¹˜ì  ì´ìš©ë§Œ í•˜ë ¤ê³  í•˜ë„¤ìš”.\", \"likes\": 1123},\n",
    "        \n",
    "        # í˜ì˜¤/ê°ˆë“± í‘œí˜„\n",
    "        {\"text\": \"ê·¸ìª½ ì‚¬ëŒë“¤ì€ í•­ìƒ ì´ëŸ° ì‹ìœ¼ë¡œ ì„ ë™í•˜ë”ë¼.\", \"likes\": 456},\n",
    "        {\"text\": \"586ë“¤ì´ ë¬¸ì œì•¼. ë§¨ë‚  ìê¸°ë“¤ ê¸°ì¤€ìœ¼ë¡œë§Œ íŒë‹¨í•¨.\", \"likes\": 345},\n",
    "        {\"text\": \"ì¢ŒíŒŒë“¤ íŠ¹ì§•: ë­ë“  ì •ì¹˜í™”ì‹œí‚´\", \"likes\": 287},\n",
    "        {\"text\": \"ìˆ˜ê¼´ë“¤ì€ ì •ë¶€ ë¹„íŒí•˜ë©´ ë‹¤ ì¢…ë¶ìœ¼ë¡œ ëª¨ë‚˜\", \"likes\": 234},\n",
    "        {\"text\": \"ìš”ì¦˜ ì Šì€ê²ƒë“¤ì€ ì™œ ê·¸ëŸ¬ëƒ\", \"likes\": 198},\n",
    "        {\"text\": \"í‹€ë”±ë“¤ì´ íˆ¬í‘œë¥¼ ì˜ëª»í•´ì„œ ì´ ì§€ê²½ì´ì•¼\", \"likes\": 176},\n",
    "        \n",
    "        # í”¼í•´ì ë¹„ë‚œ\n",
    "        {\"text\": \"ì™œ ê±°ê¸°ë¥¼ ê°”ì–´? ìœ„í—˜í•œ ì¤„ ì•Œë©´ì„œ...\", \"likes\": 123},\n",
    "        {\"text\": \"í• ë¡œìœˆì— ê·¸ë ‡ê²Œ ë§ì´ ëª¨ì´ë©´ ê·¸ë ‡ê²Œ ë˜ì§€\", \"likes\": 98},\n",
    "        \n",
    "        # ì¤‘ë¦½ì \n",
    "        {\"text\": \"ì •í™•í•œ ì‚¬ì‹¤ê´€ê³„ í™•ì¸ì´ ë¨¼ì €ì…ë‹ˆë‹¤.\", \"likes\": 567},\n",
    "        {\"text\": \"ì¬ë°œ ë°©ì§€ ëŒ€ì±…ì´ ì¤‘ìš”í•©ë‹ˆë‹¤.\", \"likes\": 489},\n",
    "        {\"text\": \"ì–‘ìª½ ë‹¤ ì§„ì •í•˜ê³  ì‚¬ì‹¤ì— ê¸°ë°˜í•´ì„œ ë…¼ì˜í•©ì‹œë‹¤.\", \"likes\": 345},\n",
    "        {\"text\": \"ì¶”ëª¨ì™€ ì§„ìƒê·œëª… ë‘˜ ë‹¤ í•„ìš”í•©ë‹ˆë‹¤.\", \"likes\": 298},\n",
    "        {\"text\": \"ê°ê´€ì ì¸ ì¡°ì‚¬ê°€ í•„ìš”í•œ ì‹œì ì…ë‹ˆë‹¤.\", \"likes\": 234},\n",
    "        {\"text\": \"ê°ì •ì  ëŒ€ì‘ë³´ë‹¤ ëƒ‰ì •í•œ ë¶„ì„ì´ í•„ìš”í•´ìš”.\", \"likes\": 198},\n",
    "    ]\n",
    "    \n",
    "    df = pd.DataFrame(sample_data)\n",
    "    df['video_id'] = 'sample_video'\n",
    "    df['video_title'] = f'{KEYWORD} ê´€ë ¨ ì˜ìƒ'\n",
    "    df['channel'] = np.random.choice(['JTBC News', 'MBC News', 'KBS News', 'SBS News'], len(df))\n",
    "    df['author'] = [f'user_{i}' for i in range(len(df))]\n",
    "    df['published_at'] = '2024-10-29'\n",
    "    df['reply_count'] = np.random.randint(0, 50, len(df))\n",
    "    \n",
    "    print(f\"âœ… ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì™„ë£Œ! ({len(df)}ê°œ ëŒ“ê¸€)\")\n",
    "    return df\n",
    "\n",
    "# ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
    "if all_comments:\n",
    "    df_comments = pd.DataFrame(all_comments)\n",
    "    print(f\"ğŸ“Š ìˆ˜ì§‘ëœ ë°ì´í„°: {len(df_comments)}ê°œ ëŒ“ê¸€\")\n",
    "else:\n",
    "    print(\"âš ï¸ ìˆ˜ì§‘ëœ ëŒ“ê¸€ì´ ì—†ì–´ ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "    df_comments = create_sample_comments()\n",
    "\n",
    "df_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 3-6. ë°ì´í„° ê¸°ë³¸ ì •ë³´ í™•ì¸\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"ğŸ“Š ë°ì´í„° ê¸°ë³¸ ì •ë³´\")\n",
    "print(\"=\"*50)\n",
    "print(f\"ì´ ëŒ“ê¸€ ìˆ˜: {len(df_comments):,}ê°œ\")\n",
    "print(f\"ë™ì˜ìƒ ìˆ˜: {df_comments['video_id'].nunique()}ê°œ\")\n",
    "print(f\"í‰ê·  ì¢‹ì•„ìš”: {df_comments['likes'].mean():.1f}ê°œ\")\n",
    "print(f\"ìµœëŒ€ ì¢‹ì•„ìš”: {df_comments['likes'].max():,}ê°œ\")\n",
    "\n",
    "print(\"\\nğŸ“º ì±„ë„ë³„ ëŒ“ê¸€ ìˆ˜:\")\n",
    "print(df_comments['channel'].value_counts().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 4-1. í˜•íƒœì†Œ ë¶„ì„ê¸° ë° ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
    "# ============================================\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "STOPWORDS = {\n",
    "    'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë•Œ', 'ì¤‘', 'ì¢€', 'ë”', 'ë„¤', 'ê±°', 'ë­', 'ì œ', 'ì €', 'ê²Œ',\n",
    "    'ì´', 'ê·¸', 'ì €', 'ì—¬ê¸°', 'ê±°ê¸°', 'ë‚˜', 'ë„ˆ', 'ìš°ë¦¬', 'ì €í¬',\n",
    "    'í•˜ë‹¤', 'ë˜ë‹¤', 'ìˆë‹¤', 'ì—†ë‹¤', 'ê°™ë‹¤', 'ë³´ë‹¤', 'ì£¼ë‹¤',\n",
    "    'ì•ˆ', 'ëª»', 'ì˜', 'ì™œ', 'ì–´ë–»ê²Œ', 'ì–¸ì œ', 'ì–´ë””', 'ëˆ„êµ¬', 'ë¬´ì—‡',\n",
    "    'ì§„ì§œ', 'ì •ë§', 'ë„ˆë¬´', 'ë§¤ìš°', 'ì•„ì£¼', 'ì™„ì „', 'ì•½ê°„', 'ì¡°ê¸ˆ',\n",
    "    'ê·¸ëƒ¥', 'ë°”ë¡œ', 'ë‹¤ì‹œ', 'ë˜', 'ì´ë¯¸', 'ì•„ì§', 'ë²Œì¨', 'ê³§',\n",
    "    'ê·¼ë°', 'ê·¸ëŸ°ë°', 'í•˜ì§€ë§Œ', 'ê·¸ë˜ì„œ', 'ê·¸ëŸ¬ë‚˜', 'ë”°ë¼ì„œ',\n",
    "    'ì˜ìƒ', 'ëŒ“ê¸€', 'êµ¬ë…', 'ì¢‹ì•„ìš”', 'ì•Œë¦¼', 'ì±„ë„',\n",
    "    'ë¶„', 'ì´ˆ', 'í¸', 'í™”', 'íšŒ', 'ë²ˆ', 'ê°œ', 'ëª…', 'ë…„', 'ì›”', 'ì¼',\n",
    "}\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"í…ìŠ¤íŠ¸ ì •ê·œí™”\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'[^ê°€-í£a-zA-Z0-9\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def extract_keywords(text, min_length=2):\n",
    "    \"\"\"ëª…ì‚¬ í‚¤ì›Œë“œ ì¶”ì¶œ\"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    nouns = okt.nouns(str(text))\n",
    "    return [w for w in nouns if len(w) >= min_length and w not in STOPWORDS]\n",
    "\n",
    "print(\"âœ… ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 4-2. ì „ì²˜ë¦¬ ì ìš©\n",
    "# ============================================\n",
    "\n",
    "print(\"ğŸ”„ ì „ì²˜ë¦¬ ì§„í–‰ ì¤‘...\")\n",
    "\n",
    "df_comments['cleaned_text'] = df_comments['text'].apply(clean_text)\n",
    "df_comments['keywords'] = df_comments['cleaned_text'].apply(extract_keywords)\n",
    "df_comments['text_length'] = df_comments['cleaned_text'].apply(len)\n",
    "\n",
    "# ë¹ˆ í…ìŠ¤íŠ¸ ì œê±°\n",
    "df_comments = df_comments[df_comments['text_length'] > 0].reset_index(drop=True)\n",
    "\n",
    "print(f\"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ! (ìœ íš¨ ëŒ“ê¸€: {len(df_comments)}ê°œ)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 4-3. ì›Œë“œí´ë¼ìš°ë“œ\n",
    "# ============================================\n",
    "\n",
    "all_keywords = []\n",
    "for keywords in df_comments['keywords']:\n",
    "    all_keywords.extend(keywords)\n",
    "\n",
    "word_freq = Counter(all_keywords)\n",
    "\n",
    "if word_freq:\n",
    "    if IN_COLAB:\n",
    "        font_path = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "    else:\n",
    "        font_path = 'C:/Windows/Fonts/malgun.ttf'\n",
    "    \n",
    "    wordcloud = WordCloud(\n",
    "        font_path=font_path,\n",
    "        width=900, height=400,\n",
    "        background_color='white',\n",
    "        colormap='RdYlBu_r',\n",
    "        max_words=80\n",
    "    ).generate_from_frequencies(word_freq)\n",
    "\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f'ğŸ“Š \"{KEYWORD}\" ìœ íŠœë¸Œ ëŒ“ê¸€ í‚¤ì›Œë“œ', fontsize=16, fontweight='bold')\n",
    "    plt.show()\n",
    "\n",
    "# TOP 20 í‚¤ì›Œë“œ\n",
    "print(\"\\nğŸ“Š TOP 20 í‚¤ì›Œë“œ:\")\n",
    "for word, count in word_freq.most_common(20):\n",
    "    bar = 'â–ˆ' * min(count // 2, 20)\n",
    "    print(f\"{word:10} | {count:3}íšŒ | {bar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. KoBERT ê°ì„±ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 5-1. KoBERT ê¸°ë°˜ ê°ì„±ë¶„ì„ ëª¨ë¸ ë¡œë“œ\n",
    "# ============================================\n",
    "\n",
    "print(\"ğŸ”„ KoBERT ê¸°ë°˜ ê°ì„±ë¶„ì„ ëª¨ë¸ ë¡œë”© ì¤‘...\")\n",
    "print(\"   (ìµœì´ˆ ì‹¤í–‰ ì‹œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œì— ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)\")\n",
    "\n",
    "# ì‚¬ìš©í•  ëª¨ë¸ ì„ íƒ\n",
    "MODEL_NAME = \"snunlp/KR-FinBert-SC\"  # 3í´ë˜ìŠ¤ (positive, negative, neutral)\n",
    "\n",
    "# ëŒ€ì•ˆ ëª¨ë¸ë“¤:\n",
    "# MODEL_NAME = \"jason9693/soongsil-bert-base-nsmc\"  # 2í´ë˜ìŠ¤\n",
    "# MODEL_NAME = \"beomi/KcELECTRA-base\"  # í•œêµ­ì–´ ëŒ“ê¸€ íŠ¹í™”\n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "    sentiment_analyzer = pipeline(\n",
    "        \"sentiment-analysis\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device=0 if torch.cuda.is_available() else -1,\n",
    "        max_length=512,\n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nâœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\")\n",
    "    print(f\"   ëª¨ë¸: {MODEL_NAME}\")\n",
    "    print(f\"   ë””ë°”ì´ìŠ¤: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
    "    MODEL_LOADED = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "    MODEL_LOADED = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 5-2. ê°ì„±ë¶„ì„ í•¨ìˆ˜ ì •ì˜\n",
    "# ============================================\n",
    "\n",
    "def analyze_sentiment_kobert(text):\n",
    "    \"\"\"\n",
    "    KoBERT ê¸°ë°˜ ê°ì„±ë¶„ì„\n",
    "    \n",
    "    Returns: (ë¼ë²¨, ì‹ ë¢°ë„, ì •ê·œí™” ì ìˆ˜)\n",
    "    \"\"\"\n",
    "    if not text or pd.isna(text) or len(str(text).strip()) < 2:\n",
    "        return 'neutral', 0.5, 0.0\n",
    "    \n",
    "    try:\n",
    "        text = str(text)[:500]\n",
    "        result = sentiment_analyzer(text)[0]\n",
    "        \n",
    "        label = result['label'].lower()\n",
    "        score = result['score']\n",
    "        \n",
    "        # ë¼ë²¨ ì •ê·œí™”\n",
    "        if 'positive' in label or 'pos' in label or label == '1':\n",
    "            return 'positive', score, score\n",
    "        elif 'negative' in label or 'neg' in label or label == '0':\n",
    "            return 'negative', score, -score\n",
    "        else:\n",
    "            return 'neutral', score, 0.0\n",
    "            \n",
    "    except:\n",
    "        return 'neutral', 0.5, 0.0\n",
    "\n",
    "# ê·œì¹™ ê¸°ë°˜ ë°±ì—…\n",
    "POSITIVE_WORDS = {'ì¢‹ë‹¤', 'ì¢‹ì•„', 'ê°ì‚¬', 'ì‘ì›', 'ì‚¬ë‘', 'í¬ë§', 'ìœ„ë¡œ', 'ëª…ë³µ', 'ì¶”ëª¨', 'í˜ë‚´'}\n",
    "NEGATIVE_WORDS = {'ì‹«ë‹¤', 'í™”ë‚˜ë‹¤', 'ë¶„ë…¸', 'ì‹¤ë§', 'ë‹µë‹µ', 'ë¬¸ì œ', 'ì˜ëª»', 'ë¬´ëŠ¥', 'ë¹„íŒ', 'ì²˜ë²Œ'}\n",
    "\n",
    "def analyze_sentiment_rule(text):\n",
    "    \"\"\"ê·œì¹™ ê¸°ë°˜ ê°ì„±ë¶„ì„ (ë°±ì—…ìš©)\"\"\"\n",
    "    if not text:\n",
    "        return 'neutral', 0.5, 0.0\n",
    "    text = str(text).lower()\n",
    "    morphs = okt.morphs(text)\n",
    "    pos = sum(1 for w in morphs if w in POSITIVE_WORDS)\n",
    "    neg = sum(1 for w in morphs if w in NEGATIVE_WORDS)\n",
    "    if pos > neg:\n",
    "        return 'positive', 0.7, pos - neg\n",
    "    elif neg > pos:\n",
    "        return 'negative', 0.7, -(neg - pos)\n",
    "    return 'neutral', 0.5, 0.0\n",
    "\n",
    "print(\"âœ… ê°ì„±ë¶„ì„ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 5-3. KoBERT ê°ì„±ë¶„ì„ í…ŒìŠ¤íŠ¸\n",
    "# ============================================\n",
    "\n",
    "if MODEL_LOADED:\n",
    "    test_texts = [\n",
    "        \"ìœ ê°€ì¡±ë¶„ë“¤ê»˜ ê¹Šì€ ìœ„ë¡œë¥¼ ì „í•©ë‹ˆë‹¤.\",\n",
    "        \"ì •ë§ í™”ê°€ ë‚©ë‹ˆë‹¤. ì±…ì„ì ì²˜ë²Œí•´ì•¼ í•©ë‹ˆë‹¤.\",\n",
    "        \"ê°ê´€ì ì¸ ì¡°ì‚¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.\",\n",
    "        \"ê·¸ìª½ ì‚¬ëŒë“¤ì€ í•­ìƒ ì´ëŸ° ì‹ì´ì•¼.\"\n",
    "    ]\n",
    "    \n",
    "    print(\"ğŸ§ª KoBERT ê°ì„±ë¶„ì„ í…ŒìŠ¤íŠ¸:\")\n",
    "    print(\"=\"*60)\n",
    "    for text in test_texts:\n",
    "        label, conf, score = analyze_sentiment_kobert(text)\n",
    "        print(f\"\\n'{text[:40]}...'\")\n",
    "        print(f\"  â†’ {label} (ì‹ ë¢°ë„: {conf:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 5-4. ì „ì²´ ë°ì´í„°ì— ê°ì„±ë¶„ì„ ì ìš©\n",
    "# ============================================\n",
    "\n",
    "print(\"ğŸ”„ KoBERT ê°ì„±ë¶„ì„ ì§„í–‰ ì¤‘...\")\n",
    "print(f\"   ì´ {len(df_comments)}ê°œ ëŒ“ê¸€ ë¶„ì„\")\n",
    "\n",
    "sentiments = []\n",
    "confidences = []\n",
    "scores = []\n",
    "\n",
    "for i, text in enumerate(df_comments['cleaned_text']):\n",
    "    if MODEL_LOADED:\n",
    "        label, conf, score = analyze_sentiment_kobert(text)\n",
    "    else:\n",
    "        label, conf, score = analyze_sentiment_rule(text)\n",
    "    \n",
    "    sentiments.append(label)\n",
    "    confidences.append(conf)\n",
    "    scores.append(score)\n",
    "    \n",
    "    if (i + 1) % 50 == 0:\n",
    "        print(f\"   ì§„í–‰: {i+1}/{len(df_comments)}\")\n",
    "\n",
    "df_comments['sentiment'] = sentiments\n",
    "df_comments['sentiment_confidence'] = confidences\n",
    "df_comments['sentiment_score'] = scores\n",
    "\n",
    "print(\"\\nâœ… ê°ì„±ë¶„ì„ ì™„ë£Œ!\")\n",
    "print(\"\\nğŸ“Š ê°ì„± ë¶„í¬:\")\n",
    "print(df_comments['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 5-5. ê°ì„±ë¶„ì„ ì‹œê°í™”\n",
    "# ============================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# 1. ê°ì„± ë¶„í¬\n",
    "sentiment_counts = df_comments['sentiment'].value_counts()\n",
    "colors = {'positive': '#27ae60', 'neutral': '#95a5a6', 'negative': '#e74c3c'}\n",
    "pie_colors = [colors.get(s, '#95a5a6') for s in sentiment_counts.index]\n",
    "\n",
    "axes[0].pie(sentiment_counts.values, \n",
    "            labels=['ê¸ì •', 'ì¤‘ë¦½', 'ë¶€ì •'][:len(sentiment_counts)],\n",
    "            autopct='%1.1f%%', colors=pie_colors, startangle=90)\n",
    "axes[0].set_title('ğŸ¤– KoBERT ê°ì„± ë¶„í¬', fontsize=12, fontweight='bold')\n",
    "\n",
    "# 2. ì‹ ë¢°ë„ ë¶„í¬\n",
    "axes[1].hist(df_comments['sentiment_confidence'], bins=20, color='steelblue',\n",
    "             edgecolor='white', alpha=0.7)\n",
    "axes[1].axvline(df_comments['sentiment_confidence'].mean(), color='red', linestyle='--',\n",
    "                label=f'í‰ê· : {df_comments[\"sentiment_confidence\"].mean():.2f}')\n",
    "axes[1].set_xlabel('ì‹ ë¢°ë„')\n",
    "axes[1].set_ylabel('ë¹ˆë„')\n",
    "axes[1].set_title('ğŸ“Š ì‹ ë¢°ë„ ë¶„í¬', fontsize=12, fontweight='bold')\n",
    "axes[1].legend()\n",
    "\n",
    "# 3. ê°ì„±ë³„ í‰ê·  ì¢‹ì•„ìš”\n",
    "sentiment_likes = df_comments.groupby('sentiment')['likes'].mean().sort_values()\n",
    "bar_colors = [colors.get(s, '#95a5a6') for s in sentiment_likes.index]\n",
    "axes[2].barh(sentiment_likes.index, sentiment_likes.values, color=bar_colors)\n",
    "axes[2].set_xlabel('í‰ê·  ì¢‹ì•„ìš”')\n",
    "axes[2].set_title('ğŸ“ˆ ê°ì„±ë³„ í‰ê·  ì¢‹ì•„ìš”', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. í˜ì˜¤í‘œí˜„ íƒì§€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 6-1. í˜ì˜¤í‘œí˜„ íŒ¨í„´ ì •ì˜\n",
    "# ============================================\n",
    "\n",
    "HATE_PATTERNS = {\n",
    "    'ì¼ë°˜í™”': [\n",
    "        'í•­ìƒ', 'ë§¨ë‚ ', 'ëŠ˜', 'ì „ë¶€', 'ë‹¤', 'ê±”ë„¤', 'ìŸ¤ë„¤', 'ê·¸ìª½', 'ì €ìª½',\n",
    "        'ë‹ˆë“¤', 'ë¬´ì¡°ê±´', 'ì–´ì°¨í”¼', 'ì—­ì‹œ', 'ê·¸ë†ˆ', 'ê·¸ë…„'\n",
    "    ],\n",
    "    'ë¹„í•˜': [\n",
    "        'í•œì‹¬', 'ë©ì²­', 'ë¬´ì‹', 'ë°”ë³´', 'ë³‘ì‹ ', 'ë˜ë¼ì´', 'ë¯¸ì¹œ',\n",
    "        'ìˆ˜ì¤€', 'ì €ì§ˆ', 'ì“°ë ˆê¸°', 'ì°Œì§ˆ', 'ê¼´í†µ', 'ë¼ì§€', 'ë²Œë ˆ'\n",
    "    ],\n",
    "    'ì„ ë™': [\n",
    "        'ê°ì„±', 'ì¼ì–´ë‚˜', 'ì‹¬íŒ', 'ë½‘ì•„ì•¼', 'ì—†ì• ì•¼', 'ë§í•´ë¼', 'ì²™ê²°',\n",
    "        'ì£½ì—¬', 'ë•Œë ¤', 'ë°•ë©¸', 'ì²­ì‚°', 'ì‘ì§•'\n",
    "    ],\n",
    "    'ì •ì¹˜í˜ì˜¤': [\n",
    "        'ì¢ŒíŒŒ', 'ìš°íŒŒ', 'ë¹¨ê°±ì´', 'ìˆ˜ê¼´', 'ê¼´í†µ', 'ì§„ë³´ì¶©', 'ë³´ìˆ˜ê¼´',\n",
    "        'ë¬¸ë¹ ', 'ìœ¤ë¹ ', 'êµ­í˜', 'ë¯¼ì£¼ë‹¹ì¶©', 'ì°ì¶©'\n",
    "    ],\n",
    "    'ì„¸ëŒ€í˜ì˜¤': [\n",
    "        'í‹€ë”±', 'ê¼°ëŒ€', '586', 'ì Šì€ê²ƒ', 'ëŠ™ì€ì´', 'ìš”ì¦˜ê²ƒë“¤', 'MZì¶©',\n",
    "        'ë¼ë–¼', 'ë…¸ì¸ë„¤', 'ì• ë“¤', 'ì–´ë¦°ê²ƒ'\n",
    "    ],\n",
    "    'ì„±ë³„í˜ì˜¤': [\n",
    "        'í˜ë¯¸', 'í•œë‚¨', 'í•œë…€', 'ë‚¨í˜', 'ì—¬í˜', 'ê¹€ì¹˜ë…€', 'ë§˜ì¶©', 'ëƒ„ì €'\n",
    "    ],\n",
    "    'í”¼í•´ìë¹„ë‚œ': [\n",
    "        'ì™œ ê°”', 'ì™œ ê±°ê¸°', 'ìì—…ìë“', 'ë‹ˆ íƒ“', 'ë‹ˆê°€ ì•Œì•„ì„œ',\n",
    "        'ê·¸ëŸ¬ë‹ˆê¹Œ', 'ì¡°ì‹¬í–ˆì–´ì•¼', 'ì•ˆ ê°”ìœ¼ë©´'\n",
    "    ]\n",
    "}\n",
    "\n",
    "def detect_hate_speech(text):\n",
    "    \"\"\"í˜ì˜¤í‘œí˜„ íƒì§€\"\"\"\n",
    "    if not text or pd.isna(text):\n",
    "        return [], [], 0\n",
    "    \n",
    "    text = str(text).lower()\n",
    "    found_types = []\n",
    "    found_words = []\n",
    "    \n",
    "    for hate_type, patterns in HATE_PATTERNS.items():\n",
    "        for pattern in patterns:\n",
    "            if pattern.lower() in text:\n",
    "                if hate_type not in found_types:\n",
    "                    found_types.append(hate_type)\n",
    "                found_words.append(pattern)\n",
    "    \n",
    "    hate_score = len(found_types) * 2 + len(found_words)\n",
    "    return found_types, found_words, hate_score\n",
    "\n",
    "print(\"âœ… í˜ì˜¤í‘œí˜„ íŒ¨í„´ ì •ì˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 6-2. í˜ì˜¤í‘œí˜„ íƒì§€ ì ìš©\n",
    "# ============================================\n",
    "\n",
    "print(\"ğŸ”„ í˜ì˜¤í‘œí˜„ íƒì§€ ì¤‘...\")\n",
    "\n",
    "hate_results = df_comments['cleaned_text'].apply(detect_hate_speech)\n",
    "\n",
    "df_comments['hate_types'] = [r[0] for r in hate_results]\n",
    "df_comments['hate_words'] = [r[1] for r in hate_results]\n",
    "df_comments['hate_score'] = [r[2] for r in hate_results]\n",
    "df_comments['has_hate'] = df_comments['hate_types'].apply(lambda x: len(x) > 0)\n",
    "\n",
    "hate_count = df_comments['has_hate'].sum()\n",
    "hate_ratio = hate_count / len(df_comments) * 100\n",
    "\n",
    "print(\"âœ… ì™„ë£Œ!\")\n",
    "print(f\"\\nğŸ“Š í˜ì˜¤í‘œí˜„ íƒì§€ ê²°ê³¼:\")\n",
    "print(f\"   ì „ì²´: {len(df_comments)}ê°œ\")\n",
    "print(f\"   í˜ì˜¤ í¬í•¨: {hate_count}ê°œ ({hate_ratio:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 6-3. í˜ì˜¤í‘œí˜„ ìœ í˜•ë³„ ë¶„ì„\n",
    "# ============================================\n",
    "\n",
    "all_hate_types = []\n",
    "for types in df_comments['hate_types']:\n",
    "    all_hate_types.extend(types)\n",
    "\n",
    "type_freq = Counter(all_hate_types)\n",
    "\n",
    "print(\"\\nğŸ“Š í˜ì˜¤í‘œí˜„ ìœ í˜•ë³„ ë¹ˆë„:\")\n",
    "print(\"=\"*50)\n",
    "for hate_type, count in type_freq.most_common():\n",
    "    pct = count / len(df_comments) * 100\n",
    "    bar = 'â–ˆ' * min(count, 20)\n",
    "    print(f\"{hate_type:12} | {count:3}ê°œ ({pct:5.1f}%) | {bar}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 6-4. í˜ì˜¤í‘œí˜„ ì‹œê°í™”\n",
    "# ============================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# 1. í˜ì˜¤ ë¹„ìœ¨\n",
    "hate_data = [df_comments['has_hate'].sum(), (~df_comments['has_hate']).sum()]\n",
    "axes[0].pie(hate_data, labels=['í˜ì˜¤ í¬í•¨', 'ì •ìƒ'], autopct='%1.1f%%',\n",
    "            colors=['#e74c3c', '#3498db'], startangle=90, explode=[0.05, 0])\n",
    "axes[0].set_title(f'âš ï¸ í˜ì˜¤í‘œí˜„ ë¹„ìœ¨ ({hate_ratio:.1f}%)', fontsize=12, fontweight='bold')\n",
    "\n",
    "# 2. ìœ í˜•ë³„ ë¶„í¬\n",
    "if type_freq:\n",
    "    types, counts = zip(*type_freq.most_common())\n",
    "    colors_hate = plt.cm.Reds(np.linspace(0.3, 0.9, len(types)))\n",
    "    axes[1].barh(types, counts, color=colors_hate)\n",
    "    axes[1].set_xlabel('ë¹ˆë„')\n",
    "    axes[1].invert_yaxis()\n",
    "axes[1].set_title('ğŸš« í˜ì˜¤í‘œí˜„ ìœ í˜•', fontsize=12, fontweight='bold')\n",
    "\n",
    "# 3. ê°ì„± vs í˜ì˜¤\n",
    "cross_tab = pd.crosstab(df_comments['sentiment'], df_comments['has_hate'])\n",
    "cross_tab.columns = ['ì •ìƒ', 'í˜ì˜¤']\n",
    "cross_tab.plot(kind='bar', ax=axes[2], color=['#3498db', '#e74c3c'], edgecolor='white')\n",
    "axes[2].set_xlabel('ê°ì„±')\n",
    "axes[2].set_ylabel('ëŒ“ê¸€ ìˆ˜')\n",
    "axes[2].set_title('ğŸ“Š ê°ì„±ë³„ í˜ì˜¤í‘œí˜„', fontsize=12, fontweight='bold')\n",
    "axes[2].tick_params(axis='x', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 7. ì¢…í•© ë¶„ì„ ë° ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 7-1. ì¢…í•© ëŒ€ì‹œë³´ë“œ\n",
    "# ============================================\n",
    "\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "\n",
    "# 1. KoBERT ê°ì„± ë¶„í¬\n",
    "ax1 = fig.add_subplot(2, 3, 1)\n",
    "sentiment_counts = df_comments['sentiment'].value_counts()\n",
    "colors_sent = {'positive': '#27ae60', 'neutral': '#95a5a6', 'negative': '#e74c3c'}\n",
    "pie_colors = [colors_sent.get(s, '#95a5a6') for s in sentiment_counts.index]\n",
    "ax1.pie(sentiment_counts.values, labels=['ê¸ì •', 'ì¤‘ë¦½', 'ë¶€ì •'][:len(sentiment_counts)],\n",
    "        autopct='%1.1f%%', colors=pie_colors, startangle=90)\n",
    "ax1.set_title('ğŸ¤– KoBERT ê°ì„± ë¶„í¬', fontsize=12, fontweight='bold')\n",
    "\n",
    "# 2. í˜ì˜¤í‘œí˜„ ë¹„ìœ¨\n",
    "ax2 = fig.add_subplot(2, 3, 2)\n",
    "hate_data = [df_comments['has_hate'].sum(), (~df_comments['has_hate']).sum()]\n",
    "ax2.pie(hate_data, labels=['í˜ì˜¤ í¬í•¨', 'ì •ìƒ'], autopct='%1.1f%%',\n",
    "        colors=['#e74c3c', '#3498db'], startangle=90, explode=[0.05, 0])\n",
    "ax2.set_title(f'âš ï¸ í˜ì˜¤í‘œí˜„ ë¹„ìœ¨ ({hate_ratio:.1f}%)', fontsize=12, fontweight='bold')\n",
    "\n",
    "# 3. ì±„ë„ë³„ ëŒ“ê¸€ ìˆ˜\n",
    "ax3 = fig.add_subplot(2, 3, 3)\n",
    "channel_counts = df_comments['channel'].value_counts().head(7)\n",
    "ax3.barh(channel_counts.index, channel_counts.values, color='steelblue')\n",
    "ax3.set_xlabel('ëŒ“ê¸€ ìˆ˜')\n",
    "ax3.invert_yaxis()\n",
    "ax3.set_title('ğŸ“º ì±„ë„ë³„ ëŒ“ê¸€ ìˆ˜', fontsize=12, fontweight='bold')\n",
    "\n",
    "# 4. í‚¤ì›Œë“œ TOP 10\n",
    "ax4 = fig.add_subplot(2, 3, 4)\n",
    "top_words = word_freq.most_common(10)\n",
    "if top_words:\n",
    "    words, freqs = zip(*top_words)\n",
    "    ax4.barh(words, freqs, color='teal')\n",
    "    ax4.set_xlabel('ë¹ˆë„')\n",
    "    ax4.invert_yaxis()\n",
    "ax4.set_title('ğŸ”‘ ì£¼ìš” í‚¤ì›Œë“œ TOP 10', fontsize=12, fontweight='bold')\n",
    "\n",
    "# 5. í˜ì˜¤ ìœ í˜•ë³„ ë¶„í¬\n",
    "ax5 = fig.add_subplot(2, 3, 5)\n",
    "if type_freq:\n",
    "    types, counts = zip(*type_freq.most_common())\n",
    "    colors_hate = plt.cm.Reds(np.linspace(0.3, 0.9, len(types)))\n",
    "    ax5.barh(types, counts, color=colors_hate)\n",
    "    ax5.set_xlabel('ë¹ˆë„')\n",
    "    ax5.invert_yaxis()\n",
    "ax5.set_title('ğŸš« í˜ì˜¤í‘œí˜„ ìœ í˜•', fontsize=12, fontweight='bold')\n",
    "\n",
    "# 6. ì‹ ë¢°ë„ vs ì¢‹ì•„ìš”\n",
    "ax6 = fig.add_subplot(2, 3, 6)\n",
    "scatter_colors = df_comments['has_hate'].map({True: '#e74c3c', False: '#3498db'})\n",
    "ax6.scatter(df_comments['sentiment_confidence'], df_comments['likes'],\n",
    "            c=scatter_colors, alpha=0.5, s=30)\n",
    "ax6.set_xlabel('ê°ì„± ì‹ ë¢°ë„')\n",
    "ax6.set_ylabel('ì¢‹ì•„ìš”')\n",
    "ax6.set_title('ğŸ“ˆ ì‹ ë¢°ë„ vs ì¢‹ì•„ìš” (ë¹¨ê°•=í˜ì˜¤)', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.suptitle(f'ğŸ“Š \"{KEYWORD}\" YouTube API + KoBERT ë¶„ì„ ê²°ê³¼',\n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 7-2. ë¶„ì„ ê²°ê³¼ ìš”ì•½\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"ğŸ“‹ '{KEYWORD}' YouTube API + KoBERT ë¶„ì„ ê²°ê³¼\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n[1] ë°ì´í„° ê°œìš”\")\n",
    "print(f\"   - ê²€ìƒ‰ í‚¤ì›Œë“œ: {KEYWORD}\")\n",
    "print(f\"   - ë¶„ì„ ë™ì˜ìƒ: {df_comments['video_id'].nunique()}ê°œ\")\n",
    "print(f\"   - ì´ ëŒ“ê¸€ ìˆ˜: {len(df_comments):,}ê°œ\")\n",
    "print(f\"   - ìˆ˜ì§‘ ë°©ì‹: YouTube Data API v3\")\n",
    "\n",
    "print(f\"\\n[2] KoBERT ê°ì„±ë¶„ì„\")\n",
    "print(f\"   - ëª¨ë¸: {MODEL_NAME if MODEL_LOADED else 'ê·œì¹™ ê¸°ë°˜'}\")\n",
    "for sentiment, count in df_comments['sentiment'].value_counts().items():\n",
    "    pct = count / len(df_comments) * 100\n",
    "    print(f\"   - {sentiment}: {count}ê°œ ({pct:.1f}%)\")\n",
    "print(f\"   - í‰ê·  ì‹ ë¢°ë„: {df_comments['sentiment_confidence'].mean():.3f}\")\n",
    "\n",
    "print(f\"\\n[3] í˜ì˜¤í‘œí˜„ ë¶„ì„\")\n",
    "print(f\"   - í˜ì˜¤ í¬í•¨: {df_comments['has_hate'].sum()}ê°œ ({df_comments['has_hate'].mean()*100:.1f}%)\")\n",
    "if type_freq:\n",
    "    top_hate = type_freq.most_common(3)\n",
    "    print(f\"   - ì£¼ìš” ìœ í˜•: {', '.join([f'{t}({c})' for t, c in top_hate])}\")\n",
    "\n",
    "print(f\"\\n[4] ì£¼ìš” í‚¤ì›Œë“œ\")\n",
    "top_kw = word_freq.most_common(10)\n",
    "print(f\"   - {', '.join([w for w, c in top_kw])}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 7-3. ê²°ê³¼ ì €ì¥\n",
    "# ============================================\n",
    "\n",
    "output_cols = ['video_title', 'channel', 'text', 'cleaned_text', 'likes',\n",
    "               'sentiment', 'sentiment_confidence', 'sentiment_score',\n",
    "               'has_hate', 'hate_types', 'hate_words', 'hate_score', 'keywords']\n",
    "\n",
    "df_output = df_comments[output_cols].copy()\n",
    "df_output['keywords'] = df_output['keywords'].apply(lambda x: ', '.join(x) if x else '')\n",
    "df_output['hate_types'] = df_output['hate_types'].apply(lambda x: ', '.join(x) if x else '')\n",
    "df_output['hate_words'] = df_output['hate_words'].apply(lambda x: ', '.join(x) if x else '')\n",
    "\n",
    "filename = f\"youtube_api_kobert_analysis_{KEYWORD.replace(' ', '_')}.csv\"\n",
    "df_output.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"âœ… ê²°ê³¼ê°€ '{filename}'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "\n",
    "# Colabì—ì„œ ë‹¤ìš´ë¡œë“œ\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(filename)\n",
    "except:\n",
    "    print(\"ë¡œì»¬ í™˜ê²½: í˜„ì¬ ë””ë ‰í† ë¦¬ì—ì„œ íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ğŸ“ í•™ìŠµ ì •ë¦¬\n",
    "\n",
    "## ì´ë²ˆ ì‹¤ìŠµì—ì„œ ë°°ìš´ ë‚´ìš©\n",
    "\n",
    "### 1. YouTube Data API v3\n",
    "- **ê³µì‹ API**ë¥¼ í†µí•œ ì•ˆì •ì ì¸ ë°ì´í„° ìˆ˜ì§‘\n",
    "- `google-api-python-client` ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©\n",
    "- ë™ì˜ìƒ ê²€ìƒ‰, ëŒ“ê¸€ ìˆ˜ì§‘ ê¸°ëŠ¥\n",
    "- ì¼ì¼ í• ë‹¹ëŸ‰: 10,000 units\n",
    "\n",
    "### 2. API vs í¬ë¡¤ë§ ë¹„êµ\n",
    "| í•­ëª© | YouTube API | í¬ë¡¤ë§ |\n",
    "|------|-------------|--------|\n",
    "| ì•ˆì •ì„± | âœ… ë†’ìŒ | âš ï¸ ë³€ë™ ê°€ëŠ¥ |\n",
    "| ì†ë„ | âœ… ë¹ ë¦„ | ğŸ¢ ëŠë¦¼ |\n",
    "| ì œí•œ | ì¼ì¼ í• ë‹¹ëŸ‰ | ì°¨ë‹¨ ìœ„í—˜ |\n",
    "| ë°ì´í„° í’ˆì§ˆ | âœ… ì •í™• | âš ï¸ íŒŒì‹± ì˜¤ë¥˜ ê°€ëŠ¥ |\n",
    "\n",
    "### 3. KoBERT ê°ì„±ë¶„ì„\n",
    "- ì‚¬ì „í•™ìŠµëœ í•œêµ­ì–´ BERT ëª¨ë¸ í™œìš©\n",
    "- Hugging Face transformers ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "- ì‹ ë¢°ë„(confidence) ì ìˆ˜ ì œê³µ\n",
    "\n",
    "### 4. í˜ì˜¤í‘œí˜„ íƒì§€\n",
    "- 7ê°€ì§€ ìœ í˜•ì˜ íŒ¨í„´ ê¸°ë°˜ íƒì§€\n",
    "- ê°ì„±ë¶„ì„ê³¼ í˜ì˜¤íƒì§€ì˜ ê´€ê³„ ë¶„ì„\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š ì°¸ê³  ìë£Œ\n",
    "\n",
    "- [YouTube Data API ë¬¸ì„œ](https://developers.google.com/youtube/v3)\n",
    "- [Google Cloud Console](https://console.cloud.google.com/)\n",
    "- [Hugging Face Korean Models](https://huggingface.co/models?language=ko)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
