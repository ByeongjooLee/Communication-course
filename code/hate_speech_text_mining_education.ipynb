{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ” í˜ì˜¤ì™€ ê°ˆë“±ì€ ë§Œë“¤ì–´ì§€ëŠ”ê°€?\n",
    "## í…ìŠ¤íŠ¸ ë§ˆì´ë‹ì„ í™œìš©í•œ ì˜¨ë¼ì¸ ë‹´ë¡  ë¶„ì„ êµìœ¡\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“š í•™ìŠµ ëª©í‘œ\n",
    "1. ì˜¨ë¼ì¸ í…ìŠ¤íŠ¸ ë°ì´í„° ìˆ˜ì§‘ ë° ì „ì²˜ë¦¬ ë°©ë²• ì´í•´\n",
    "2. ê°ì„± ë¶„ì„ì„ í†µí•œ í˜ì˜¤ í‘œí˜„ íƒì§€\n",
    "3. í† í”½ ëª¨ë¸ë§ìœ¼ë¡œ ê°ˆë“± ì£¼ì œ íŒŒì•…\n",
    "4. ì‹œê°í™”ë¥¼ í†µí•œ íŒ¨í„´ ë°œê²¬\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
    "!pip install konlpy pandas numpy matplotlib seaborn wordcloud scikit-learn\n",
    "!pip install transformers torch\n",
    "!pip install pyLDAvis gensim\n",
    "\n",
    "# KoNLPyë¥¼ ìœ„í•œ Java ì„¤ì • (Colab)\n",
    "!apt-get update\n",
    "!apt-get install -y openjdk-11-jdk\n",
    "import os\n",
    "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-11-openjdk-amd64'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì„¤ì • (Colab)\n",
    "!apt-get install -y fonts-nanum\n",
    "!fc-cache -fv\n",
    "\n",
    "plt.rcParams['font.family'] = 'NanumBarunGothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(\"âœ… í™˜ê²½ ì„¤ì • ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: ìƒ˜í”Œ ë°ì´í„° ìƒì„±\n",
    "\n",
    "ì‹¤ì œ êµìœ¡ì—ì„œëŠ” ì›¹ í¬ë¡¤ë§ìœ¼ë¡œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” êµìœ¡ìš© ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "âš ï¸ **ìœ¤ë¦¬ì  ê³ ë ¤ì‚¬í•­**: ì‹¤ì œ ë°ì´í„° ìˆ˜ì§‘ ì‹œ ê°œì¸ì •ë³´ ë³´í˜¸ì™€ ì €ì‘ê¶Œì„ ë°˜ë“œì‹œ ì¤€ìˆ˜í•´ì•¼ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# êµìœ¡ìš© ìƒ˜í”Œ ëŒ“ê¸€ ë°ì´í„° ìƒì„±\n",
    "# ì‹¤ì œ ë¶„ì„ì—ì„œëŠ” ë‰´ìŠ¤ API, YouTube API ë“±ì„ í™œìš©\n",
    "\n",
    "sample_comments = [\n",
    "    # ì¤‘ë¦½ì  ì˜ê²¬\n",
    "    {\"text\": \"ì´ ë¬¸ì œëŠ” ë‹¤ì–‘í•œ ê´€ì ì—ì„œ ë³¼ í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤\", \"source\": \"ë‰´ìŠ¤ëŒ“ê¸€\", \"topic\": \"ì •ì±…\"},\n",
    "    {\"text\": \"ê°ê´€ì ì¸ ë°ì´í„°ë¥¼ ë³´ê³  íŒë‹¨í•´ì•¼ í•  ê²ƒ ê°™ì•„ìš”\", \"source\": \"ë‰´ìŠ¤ëŒ“ê¸€\", \"topic\": \"ì •ì±…\"},\n",
    "    {\"text\": \"ì–‘ìª½ ì˜ê²¬ ëª¨ë‘ ì¼ë¦¬ê°€ ìˆë„¤ìš”\", \"source\": \"ìœ íŠœë¸Œ\", \"topic\": \"ì‚¬íšŒ\"},\n",
    "    {\"text\": \"ë” ë§ì€ ì •ë³´ê°€ í•„ìš”í•œ ê²ƒ ê°™ìŠµë‹ˆë‹¤\", \"source\": \"ë‰´ìŠ¤ëŒ“ê¸€\", \"topic\": \"ê²½ì œ\"},\n",
    "    {\"text\": \"ì „ë¬¸ê°€ ì˜ê²¬ì„ ë“¤ì–´ë´ì•¼ê² ì–´ìš”\", \"source\": \"ìœ íŠœë¸Œ\", \"topic\": \"ì •ì±…\"},\n",
    "    \n",
    "    # ê±´ì„¤ì  ë¹„íŒ\n",
    "    {\"text\": \"ì´ ì •ì±…ì€ ì¬ê²€í† ê°€ í•„ìš”í•´ ë³´ì…ë‹ˆë‹¤\", \"source\": \"ë‰´ìŠ¤ëŒ“ê¸€\", \"topic\": \"ì •ì±…\"},\n",
    "    {\"text\": \"ë…¼ë¦¬ì ìœ¼ë¡œ ëª‡ ê°€ì§€ ë¬¸ì œì ì´ ìˆì–´ìš”\", \"source\": \"ë‰´ìŠ¤ëŒ“ê¸€\", \"topic\": \"ì‚¬íšŒ\"},\n",
    "    {\"text\": \"ëŒ€ì•ˆì„ í•¨ê»˜ ì œì‹œí•´ì£¼ì‹œë©´ ì¢‹ê² ìŠµë‹ˆë‹¤\", \"source\": \"ìœ íŠœë¸Œ\", \"topic\": \"ê²½ì œ\"},\n",
    "    {\"text\": \"ì´ ë¶€ë¶„ì€ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤\", \"source\": \"ì»¤ë®¤ë‹ˆí‹°\", \"topic\": \"ì •ì±…\"},\n",
    "    {\"text\": \"ê·¼ê±°ë¥¼ ì¢€ ë” ëª…í™•íˆ í•´ì£¼ì„¸ìš”\", \"source\": \"ë‰´ìŠ¤ëŒ“ê¸€\", \"topic\": \"ì‚¬íšŒ\"},\n",
    "    \n",
    "    # ë¶€ì •ì /ê³µê²©ì  í‘œí˜„ (ë¶„ì„ ëŒ€ìƒ)\n",
    "    {\"text\": \"ì´ê±´ ì™„ì „ ë§ë„ ì•ˆ ë˜ëŠ” ì†Œë¦¬ì•¼\", \"source\": \"ìœ íŠœë¸Œ\", \"topic\": \"ì •ì±…\"},\n",
    "    {\"text\": \"ë„ëŒ€ì²´ ë­˜ í•˜ëŠ” ê±´ì§€ ëª¨ë¥´ê² ë‹¤\", \"source\": \"ë‰´ìŠ¤ëŒ“ê¸€\", \"topic\": \"ì •ì±…\"},\n",
    "    {\"text\": \"ì´ëŸ° ê¸°ì‚¬ ì™œ ì“°ëŠ”ì§€ ì´í•´ê°€ ì•ˆ ë¨\", \"source\": \"ë‰´ìŠ¤ëŒ“ê¸€\", \"topic\": \"ì–¸ë¡ \"},\n",
    "    {\"text\": \"ê¸°ìê°€ í¸íŒŒì ì´ë„¤ìš”\", \"source\": \"ë‰´ìŠ¤ëŒ“ê¸€\", \"topic\": \"ì–¸ë¡ \"},\n",
    "    {\"text\": \"í•œìª½ ì–˜ê¸°ë§Œ í•˜ê³  ìˆì–ì•„\", \"source\": \"ìœ íŠœë¸Œ\", \"topic\": \"ì–¸ë¡ \"},\n",
    "    \n",
    "    # ê°ˆë“± ìœ ë°œì„± í‘œí˜„\n",
    "    {\"text\": \"ê·¸ìª½ ì‚¬ëŒë“¤ì€ í•­ìƒ ì´ëŸ° ì‹ì´ì•¼\", \"source\": \"ì»¤ë®¤ë‹ˆí‹°\", \"topic\": \"ì‚¬íšŒ\"},\n",
    "    {\"text\": \"ì—­ì‹œ ì˜ˆìƒëŒ€ë¡œë„¤\", \"source\": \"ìœ íŠœë¸Œ\", \"topic\": \"ì •ì±…\"},\n",
    "    {\"text\": \"ì´ê²Œ ë‚˜ë¼ëƒ\", \"source\": \"ë‰´ìŠ¤ëŒ“ê¸€\", \"topic\": \"ì •ì±…\"},\n",
    "    {\"text\": \"êµ­ë¯¼ì„ ë­˜ë¡œ ë³´ëŠ” ê±°ì•¼\", \"source\": \"ë‰´ìŠ¤ëŒ“ê¸€\", \"topic\": \"ì •ì±…\"},\n",
    "    {\"text\": \"ì§„ì§œ ë‹µì´ ì—†ë‹¤\", \"source\": \"ì»¤ë®¤ë‹ˆí‹°\", \"topic\": \"ì‚¬íšŒ\"},\n",
    "    \n",
    "    # ê¸ì •ì  í‘œí˜„\n",
    "    {\"text\": \"ì¢‹ì€ ì •ë³´ ê°ì‚¬í•©ë‹ˆë‹¤\", \"source\": \"ìœ íŠœë¸Œ\", \"topic\": \"ì •ë³´\"},\n",
    "    {\"text\": \"ì´ëŸ° ë¶„ì„ ê¸°ì‚¬ ì¢‹ì•„ìš”\", \"source\": \"ë‰´ìŠ¤ëŒ“ê¸€\", \"topic\": \"ì–¸ë¡ \"},\n",
    "    {\"text\": \"ì‘ì›í•©ë‹ˆë‹¤ í˜ë‚´ì„¸ìš”\", \"source\": \"ìœ íŠœë¸Œ\", \"topic\": \"ì‚¬íšŒ\"},\n",
    "    {\"text\": \"ì¢‹ì€ ë°©í–¥ìœ¼ë¡œ ê°€ê³  ìˆë„¤ìš”\", \"source\": \"ë‰´ìŠ¤ëŒ“ê¸€\", \"topic\": \"ì •ì±…\"},\n",
    "    {\"text\": \"ê¸°ëŒ€ë©ë‹ˆë‹¤\", \"source\": \"ì»¤ë®¤ë‹ˆí‹°\", \"topic\": \"ê²½ì œ\"},\n",
    "]\n",
    "\n",
    "# DataFrame ìƒì„±\n",
    "df = pd.DataFrame(sample_comments)\n",
    "df['id'] = range(1, len(df) + 1)\n",
    "df['date'] = pd.date_range(start='2024-01-01', periods=len(df), freq='D')\n",
    "\n",
    "print(f\"ğŸ“Š ì´ {len(df)}ê°œì˜ ìƒ˜í”Œ ëŒ“ê¸€ ë°ì´í„° ìƒì„±\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "# í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”\n",
    "okt = Okt()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜\"\"\"\n",
    "    # íŠ¹ìˆ˜ë¬¸ì ì œê±° (í•œê¸€, ì˜ë¬¸, ìˆ«ìë§Œ ìœ ì§€)\n",
    "    text = re.sub(r'[^ê°€-í£a-zA-Z0-9\\s]', '', text)\n",
    "    # ì—°ì† ê³µë°± ì œê±°\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def extract_keywords(text):\n",
    "    \"\"\"ëª…ì‚¬ í‚¤ì›Œë“œ ì¶”ì¶œ\"\"\"\n",
    "    nouns = okt.nouns(text)\n",
    "    # í•œ ê¸€ì ëª…ì‚¬ ì œì™¸\n",
    "    return [noun for noun in nouns if len(noun) > 1]\n",
    "\n",
    "def extract_morphs(text):\n",
    "    \"\"\"í˜•íƒœì†Œ ë¶„ì„ (ëª…ì‚¬, í˜•ìš©ì‚¬, ë™ì‚¬)\"\"\"\n",
    "    morphs = okt.pos(text)\n",
    "    return [word for word, pos in morphs if pos in ['Noun', 'Adjective', 'Verb'] and len(word) > 1]\n",
    "\n",
    "# ì „ì²˜ë¦¬ ì ìš©\n",
    "df['cleaned_text'] = df['text'].apply(preprocess_text)\n",
    "df['keywords'] = df['cleaned_text'].apply(extract_keywords)\n",
    "df['morphs'] = df['cleaned_text'].apply(extract_morphs)\n",
    "\n",
    "print(\"âœ… í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì™„ë£Œ!\")\n",
    "df[['text', 'cleaned_text', 'keywords']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: ê°ì„± ë¶„ì„ (Sentiment Analysis)\n",
    "\n",
    "### 4.1 ê·œì¹™ ê¸°ë°˜ ê°ì„± ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê°ì„± ì‚¬ì „ ì •ì˜ (êµìœ¡ìš© ê°„ì†Œí™” ë²„ì „)\n",
    "positive_words = [\n",
    "    'ì¢‹ë‹¤', 'ì¢‹ì•„ìš”', 'ì¢‹ì€', 'ê°ì‚¬', 'ì‘ì›', 'ê¸°ëŒ€', 'í›Œë¥­', 'ìµœê³ ', \n",
    "    'ê¸ì •', 'í¬ë§', 'ë°œì „', 'ê°œì„ ', 'í˜‘ë ¥', 'ì´í•´', 'ì¡´ì¤‘', 'ê³µê°'\n",
    "]\n",
    "\n",
    "negative_words = [\n",
    "    'ì‹«ë‹¤', 'ë‚˜ì˜ë‹¤', 'ë¬¸ì œ', 'ê±±ì •', 'ë¶ˆë§Œ', 'ì‹¤ë§', 'ë‹µë‹µ', 'í™”ë‚˜',\n",
    "    'ë§ë„ ì•ˆ', 'ì´í•´ ì•ˆ', 'ëª¨ë¥´ê² ', 'ë‹µì´ ì—†', 'í•œì‹¬', 'í¸íŒŒ'\n",
    "]\n",
    "\n",
    "conflict_words = [\n",
    "    'ê·¸ìª½', 'ë„ˆí¬', 'ê±”ë„¤', 'í•­ìƒ', 'ì—­ì‹œ', 'ë»”í•˜', 'ê·¸ëŸ¬ë‹ˆê¹Œ', \n",
    "    'ì´ê²Œ ë‚˜ë¼', 'êµ­ë¯¼ì„', 'ì´ëŸ° ì‹'\n",
    "]\n",
    "\n",
    "def analyze_sentiment_rule(text):\n",
    "    \"\"\"ê·œì¹™ ê¸°ë°˜ ê°ì„± ë¶„ì„\"\"\"\n",
    "    pos_count = sum(1 for word in positive_words if word in text)\n",
    "    neg_count = sum(1 for word in negative_words if word in text)\n",
    "    conflict_count = sum(1 for word in conflict_words if word in text)\n",
    "    \n",
    "    # ì ìˆ˜ ê³„ì‚°\n",
    "    score = pos_count - neg_count - (conflict_count * 1.5)\n",
    "    \n",
    "    if score > 0:\n",
    "        return 'positive', score\n",
    "    elif score < 0:\n",
    "        return 'negative', score\n",
    "    else:\n",
    "        return 'neutral', score\n",
    "\n",
    "def detect_conflict_potential(text):\n",
    "    \"\"\"ê°ˆë“± ìœ ë°œ ê°€ëŠ¥ì„± íƒì§€\"\"\"\n",
    "    conflict_count = sum(1 for word in conflict_words if word in text)\n",
    "    if conflict_count >= 2:\n",
    "        return 'high'\n",
    "    elif conflict_count == 1:\n",
    "        return 'medium'\n",
    "    else:\n",
    "        return 'low'\n",
    "\n",
    "# ë¶„ì„ ì ìš©\n",
    "df['sentiment_result'] = df['text'].apply(analyze_sentiment_rule)\n",
    "df['sentiment'] = df['sentiment_result'].apply(lambda x: x[0])\n",
    "df['sentiment_score'] = df['sentiment_result'].apply(lambda x: x[1])\n",
    "df['conflict_potential'] = df['text'].apply(detect_conflict_potential)\n",
    "\n",
    "print(\"âœ… ê°ì„± ë¶„ì„ ì™„ë£Œ!\")\n",
    "df[['text', 'sentiment', 'sentiment_score', 'conflict_potential']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 ë”¥ëŸ¬ë‹ ê¸°ë°˜ ê°ì„± ë¶„ì„ (KoBERT í™œìš©)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Faceì˜ í•œêµ­ì–´ ê°ì„± ë¶„ì„ ëª¨ë¸ í™œìš©\n",
    "from transformers import pipeline\n",
    "\n",
    "try:\n",
    "    # í•œêµ­ì–´ ê°ì„± ë¶„ì„ íŒŒì´í”„ë¼ì¸\n",
    "    sentiment_pipeline = pipeline(\n",
    "        \"sentiment-analysis\",\n",
    "        model=\"snunlp/KR-FinBert-SC\",  # í•œêµ­ì–´ ê°ì„± ë¶„ì„ ëª¨ë¸\n",
    "        tokenizer=\"snunlp/KR-FinBert-SC\"\n",
    "    )\n",
    "    \n",
    "    def analyze_sentiment_dl(text):\n",
    "        \"\"\"ë”¥ëŸ¬ë‹ ê¸°ë°˜ ê°ì„± ë¶„ì„\"\"\"\n",
    "        try:\n",
    "            result = sentiment_pipeline(text[:512])[0]  # ìµœëŒ€ 512 í† í°\n",
    "            return result['label'], result['score']\n",
    "        except:\n",
    "            return 'unknown', 0.0\n",
    "    \n",
    "    # ì¼ë¶€ ìƒ˜í”Œì— ì ìš© (ì‹œê°„ ì†Œìš”ë¡œ ì¸í•´)\n",
    "    sample_texts = df['text'].head(5).tolist()\n",
    "    for text in sample_texts:\n",
    "        label, score = analyze_sentiment_dl(text)\n",
    "        print(f\"í…ìŠ¤íŠ¸: {text[:30]}... â†’ ê°ì„±: {label} (ì‹ ë¢°ë„: {score:.2f})\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ ë”¥ëŸ¬ë‹ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "    print(\"ê·œì¹™ ê¸°ë°˜ ë¶„ì„ ê²°ê³¼ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: ì‹œê°í™” ë° íŒ¨í„´ ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹œê°í™” ì„¤ì •\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. ê°ì„± ë¶„í¬\n",
    "sentiment_counts = df['sentiment'].value_counts()\n",
    "colors = {'positive': '#4CAF50', 'neutral': '#FFC107', 'negative': '#F44336'}\n",
    "axes[0, 0].pie(sentiment_counts.values, labels=sentiment_counts.index, \n",
    "               autopct='%1.1f%%', colors=[colors.get(s, '#999') for s in sentiment_counts.index])\n",
    "axes[0, 0].set_title('ğŸ“Š ì „ì²´ ê°ì„± ë¶„í¬', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 2. ì¶œì²˜ë³„ ê°ì„± ë¶„í¬\n",
    "source_sentiment = pd.crosstab(df['source'], df['sentiment'], normalize='index') * 100\n",
    "source_sentiment.plot(kind='bar', ax=axes[0, 1], color=['#F44336', '#FFC107', '#4CAF50'])\n",
    "axes[0, 1].set_title('ğŸ“± ì¶œì²˜ë³„ ê°ì„± ë¶„í¬', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('ë¹„ìœ¨ (%)')\n",
    "axes[0, 1].set_xticklabels(axes[0, 1].get_xticklabels(), rotation=45)\n",
    "axes[0, 1].legend(title='ê°ì„±')\n",
    "\n",
    "# 3. ê°ˆë“± ìœ ë°œ ê°€ëŠ¥ì„± ë¶„í¬\n",
    "conflict_counts = df['conflict_potential'].value_counts()\n",
    "conflict_colors = {'low': '#4CAF50', 'medium': '#FFC107', 'high': '#F44336'}\n",
    "axes[1, 0].bar(conflict_counts.index, conflict_counts.values, \n",
    "               color=[conflict_colors.get(c, '#999') for c in conflict_counts.index])\n",
    "axes[1, 0].set_title('âš ï¸ ê°ˆë“± ìœ ë°œ ê°€ëŠ¥ì„± ë¶„í¬', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('ëŒ“ê¸€ ìˆ˜')\n",
    "\n",
    "# 4. ì£¼ì œë³„ ë¶€ì •ì  ê°ì„± ë¹„ìœ¨\n",
    "topic_negative = df[df['sentiment'] == 'negative'].groupby('topic').size()\n",
    "topic_total = df.groupby('topic').size()\n",
    "negative_ratio = (topic_negative / topic_total * 100).sort_values(ascending=True)\n",
    "negative_ratio.plot(kind='barh', ax=axes[1, 1], color='#F44336')\n",
    "axes[1, 1].set_title('ğŸ“ˆ ì£¼ì œë³„ ë¶€ì •ì  ê°ì„± ë¹„ìœ¨', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('ë¶€ì •ì  ê°ì„± ë¹„ìœ¨ (%)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('sentiment_analysis_result.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ“Š ì‹œê°í™” ì €ì¥ ì™„ë£Œ: sentiment_analysis_result.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# ì „ì²´ í‚¤ì›Œë“œ ìˆ˜ì§‘\n",
    "all_keywords = []\n",
    "for keywords in df['keywords']:\n",
    "    all_keywords.extend(keywords)\n",
    "\n",
    "# ë¹ˆë„ìˆ˜ ê³„ì‚°\n",
    "keyword_freq = Counter(all_keywords)\n",
    "\n",
    "# ê°ì„±ë³„ í‚¤ì›Œë“œ ë¶„ë¦¬\n",
    "positive_keywords = []\n",
    "negative_keywords = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    if row['sentiment'] == 'positive':\n",
    "        positive_keywords.extend(row['keywords'])\n",
    "    elif row['sentiment'] == 'negative':\n",
    "        negative_keywords.extend(row['keywords'])\n",
    "\n",
    "# ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± í•¨ìˆ˜\n",
    "def create_wordcloud(word_freq, title, colormap='viridis'):\n",
    "    # ë‚˜ëˆ”ê³ ë”• í°íŠ¸ ê²½ë¡œ (Colab)\n",
    "    font_path = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "    \n",
    "    wc = WordCloud(\n",
    "        font_path=font_path,\n",
    "        width=800,\n",
    "        height=400,\n",
    "        background_color='white',\n",
    "        colormap=colormap,\n",
    "        max_words=100\n",
    "    ).generate_from_frequencies(dict(word_freq))\n",
    "    \n",
    "    return wc\n",
    "\n",
    "# ì‹œê°í™”\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# ì „ì²´ ì›Œë“œí´ë¼ìš°ë“œ\n",
    "wc_all = create_wordcloud(keyword_freq, 'ì „ì²´', 'viridis')\n",
    "axes[0].imshow(wc_all, interpolation='bilinear')\n",
    "axes[0].set_title('ğŸ”¤ ì „ì²´ í‚¤ì›Œë“œ', fontsize=14, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# ê¸ì • ì›Œë“œí´ë¼ìš°ë“œ\n",
    "if positive_keywords:\n",
    "    wc_pos = create_wordcloud(Counter(positive_keywords), 'ê¸ì •', 'Greens')\n",
    "    axes[1].imshow(wc_pos, interpolation='bilinear')\n",
    "axes[1].set_title('ğŸ˜Š ê¸ì •ì  ëŒ“ê¸€ í‚¤ì›Œë“œ', fontsize=14, fontweight='bold')\n",
    "axes[1].axis('off')\n",
    "\n",
    "# ë¶€ì • ì›Œë“œí´ë¼ìš°ë“œ\n",
    "if negative_keywords:\n",
    "    wc_neg = create_wordcloud(Counter(negative_keywords), 'ë¶€ì •', 'Reds')\n",
    "    axes[2].imshow(wc_neg, interpolation='bilinear')\n",
    "axes[2].set_title('ğŸ˜  ë¶€ì •ì  ëŒ“ê¸€ í‚¤ì›Œë“œ', fontsize=14, fontweight='bold')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('wordcloud_result.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ“Š ìƒìœ„ 10ê°œ í‚¤ì›Œë“œ:\")\n",
    "for word, count in keyword_freq.most_common(10):\n",
    "    print(f\"  {word}: {count}íšŒ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: í† í”½ ëª¨ë¸ë§ (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# í˜•íƒœì†Œ ë¶„ì„ëœ í…ìŠ¤íŠ¸ ì¤€ë¹„\n",
    "df['morphs_text'] = df['morphs'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# CountVectorizerë¡œ ë¬¸ì„œ-ë‹¨ì–´ í–‰ë ¬ ìƒì„±\n",
    "vectorizer = CountVectorizer(max_features=1000, min_df=1)\n",
    "doc_term_matrix = vectorizer.fit_transform(df['morphs_text'])\n",
    "\n",
    "# LDA ëª¨ë¸ í•™ìŠµ\n",
    "n_topics = 4\n",
    "lda_model = LatentDirichletAllocation(\n",
    "    n_components=n_topics,\n",
    "    random_state=42,\n",
    "    max_iter=10\n",
    ")\n",
    "lda_model.fit(doc_term_matrix)\n",
    "\n",
    "# í† í”½ë³„ ì£¼ìš” ë‹¨ì–´ ì¶œë ¥\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"\\nğŸ“‘ í† í”½ ëª¨ë¸ë§ ê²°ê³¼\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "topic_labels = ['ì •ì±… ë¹„íŒ', 'ì–¸ë¡  ì‹ ë¢°', 'ì‚¬íšŒ ê°ˆë“±', 'ê¸ì •ì  ë°˜ì‘']\n",
    "\n",
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "    top_words_idx = topic.argsort()[-8:][::-1]\n",
    "    top_words = [feature_names[i] for i in top_words_idx]\n",
    "    print(f\"\\ní† í”½ {topic_idx + 1} ({topic_labels[topic_idx]}):\")\n",
    "    print(f\"  ì£¼ìš” í‚¤ì›Œë“œ: {', '.join(top_words)}\")\n",
    "\n",
    "# ê° ë¬¸ì„œì˜ ì£¼ìš” í† í”½ í• ë‹¹\n",
    "doc_topics = lda_model.transform(doc_term_matrix)\n",
    "df['main_topic'] = doc_topics.argmax(axis=1) + 1\n",
    "df['topic_confidence'] = doc_topics.max(axis=1)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"âœ… í† í”½ ëª¨ë¸ë§ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: í˜ì˜¤ í‘œí˜„ íŒ¨í„´ ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í˜ì˜¤ í‘œí˜„ íŒ¨í„´ ì‚¬ì „ (êµìœ¡ìš© - ì‹¤ì œ ì—°êµ¬ì—ì„œëŠ” ë” ì •êµí•œ ì‚¬ì „ ì‚¬ìš©)\n",
    "hate_patterns = {\n",
    "    'ì¼ë°˜í™”': ['í•­ìƒ', 'ê±”ë„¤', 'ê·¸ìª½', 'ë„ˆí¬', 'ë‹¤ë“¤', 'ëª¨ë“ '],\n",
    "    'ë¹„í•˜': ['í•œì‹¬', 'ë©ì²­', 'ë¬´ì‹', 'ì €ì§ˆ'],\n",
    "    'ì„ ë™': ['ê°ì„±', 'ë¶„ë…¸', 'ì¼ì–´ë‚˜', 'ì‹¬íŒ'],\n",
    "    'í¸ê°€ë¥´ê¸°': ['~ì¶©', '~ë†ˆ', '~ë…„', 'ì¢ŒíŒŒ', 'ìš°íŒŒ', 'ì§„ë³´', 'ë³´ìˆ˜']\n",
    "}\n",
    "\n",
    "def detect_hate_patterns(text):\n",
    "    \"\"\"í˜ì˜¤ í‘œí˜„ íŒ¨í„´ íƒì§€\"\"\"\n",
    "    detected = []\n",
    "    for pattern_type, keywords in hate_patterns.items():\n",
    "        for keyword in keywords:\n",
    "            if keyword in text:\n",
    "                detected.append((pattern_type, keyword))\n",
    "    return detected\n",
    "\n",
    "# íŒ¨í„´ íƒì§€ ì ìš©\n",
    "df['hate_patterns'] = df['text'].apply(detect_hate_patterns)\n",
    "df['has_hate_pattern'] = df['hate_patterns'].apply(lambda x: len(x) > 0)\n",
    "\n",
    "# ê²°ê³¼ ë¶„ì„\n",
    "print(\"ğŸ” í˜ì˜¤ í‘œí˜„ íŒ¨í„´ ë¶„ì„ ê²°ê³¼\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nì „ì²´ ëŒ“ê¸€ ìˆ˜: {len(df)}\")\n",
    "print(f\"í˜ì˜¤ íŒ¨í„´ í¬í•¨ ëŒ“ê¸€: {df['has_hate_pattern'].sum()}ê°œ ({df['has_hate_pattern'].mean()*100:.1f}%)\")\n",
    "\n",
    "# íŒ¨í„´ ìœ í˜•ë³„ ë¹ˆë„\n",
    "pattern_counts = {}\n",
    "for patterns in df['hate_patterns']:\n",
    "    for pattern_type, _ in patterns:\n",
    "        pattern_counts[pattern_type] = pattern_counts.get(pattern_type, 0) + 1\n",
    "\n",
    "print(\"\\níŒ¨í„´ ìœ í˜•ë³„ ë¹ˆë„:\")\n",
    "for pattern_type, count in sorted(pattern_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {pattern_type}: {count}íšŒ\")\n",
    "\n",
    "# í˜ì˜¤ íŒ¨í„´ í¬í•¨ ëŒ“ê¸€ ì˜ˆì‹œ\n",
    "print(\"\\ní˜ì˜¤ íŒ¨í„´ í¬í•¨ ëŒ“ê¸€ ì˜ˆì‹œ:\")\n",
    "hate_comments = df[df['has_hate_pattern']]\n",
    "for _, row in hate_comments.head(3).iterrows():\n",
    "    print(f\"  - \\\"{row['text']}\\\"\")\n",
    "    print(f\"    íƒì§€ëœ íŒ¨í„´: {row['hate_patterns']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“Š í…ìŠ¤íŠ¸ ë§ˆì´ë‹ ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n1ï¸âƒ£ ë°ì´í„° ê°œìš”\")\n",
    "print(f\"   - ë¶„ì„ ëŒ€ìƒ: {len(df)}ê°œ ëŒ“ê¸€\")\n",
    "print(f\"   - ì¶œì²˜ ë¶„í¬: {dict(df['source'].value_counts())}\")\n",
    "print(f\"   - ì£¼ì œ ë¶„í¬: {dict(df['topic'].value_counts())}\")\n",
    "\n",
    "print(\"\\n2ï¸âƒ£ ê°ì„± ë¶„ì„ ê²°ê³¼\")\n",
    "sentiment_dist = df['sentiment'].value_counts(normalize=True) * 100\n",
    "for sentiment, pct in sentiment_dist.items():\n",
    "    print(f\"   - {sentiment}: {pct:.1f}%\")\n",
    "\n",
    "print(\"\\n3ï¸âƒ£ ê°ˆë“± ìœ ë°œ ê°€ëŠ¥ì„±\")\n",
    "conflict_dist = df['conflict_potential'].value_counts(normalize=True) * 100\n",
    "for level, pct in conflict_dist.items():\n",
    "    print(f\"   - {level}: {pct:.1f}%\")\n",
    "\n",
    "print(\"\\n4ï¸âƒ£ ì£¼ìš” ë°œê²¬ì‚¬í•­\")\n",
    "print(f\"   - ê°€ì¥ ë¶€ì •ì ì¸ ì£¼ì œ: {df[df['sentiment']=='negative']['topic'].mode().values[0] if len(df[df['sentiment']=='negative']) > 0 else 'N/A'}\")\n",
    "print(f\"   - ê°€ì¥ ê°ˆë“± ìœ ë°œ ê°€ëŠ¥ì„± ë†’ì€ ì¶œì²˜: {df[df['conflict_potential']=='high']['source'].mode().values[0] if len(df[df['conflict_potential']=='high']) > 0 else 'N/A'}\")\n",
    "print(f\"   - í˜ì˜¤ íŒ¨í„´ íƒì§€ìœ¨: {df['has_hate_pattern'].mean()*100:.1f}%\")\n",
    "\n",
    "print(\"\\n5ï¸âƒ£ êµìœ¡ì  ì‹œì‚¬ì \")\n",
    "print(\"   - ì˜¨ë¼ì¸ ëŒ“ê¸€ì—ì„œ ê°ˆë“±ì€ íŠ¹ì • íŒ¨í„´ê³¼ í‘œí˜„ì„ í†µí•´ í˜•ì„±/í™•ì‚°ë¨\")\n",
    "print(\"   - ì¼ë°˜í™”, ë¹„í•˜, í¸ê°€ë¥´ê¸° í‘œí˜„ì´ ê°ˆë“± ì¦í­ì— ê¸°ì—¬\")\n",
    "print(\"   - í…ìŠ¤íŠ¸ ë§ˆì´ë‹ìœ¼ë¡œ ì´ëŸ¬í•œ íŒ¨í„´ì„ ê°ê´€ì ìœ¼ë¡œ ë¶„ì„ ê°€ëŠ¥\")\n",
    "print(\"   - ë¯¸ë””ì–´ ë¦¬í„°ëŸ¬ì‹œ êµìœ¡ì— ë°ì´í„° ê¸°ë°˜ ì ‘ê·¼ í•„ìš”\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: ê²°ê³¼ ì €ì¥ ë° ë‚´ë³´ë‚´ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¶„ì„ ê²°ê³¼ CSV ì €ì¥\n",
    "export_df = df[['id', 'text', 'source', 'topic', 'sentiment', 'sentiment_score', \n",
    "                'conflict_potential', 'main_topic', 'has_hate_pattern']]\n",
    "export_df.to_csv('analysis_result.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "# ìš”ì•½ í†µê³„ ì €ì¥\n",
    "summary_stats = {\n",
    "    'total_comments': len(df),\n",
    "    'positive_ratio': (df['sentiment'] == 'positive').mean(),\n",
    "    'negative_ratio': (df['sentiment'] == 'negative').mean(),\n",
    "    'high_conflict_ratio': (df['conflict_potential'] == 'high').mean(),\n",
    "    'hate_pattern_ratio': df['has_hate_pattern'].mean()\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('summary_stats.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary_stats, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"âœ… ê²°ê³¼ íŒŒì¼ ì €ì¥ ì™„ë£Œ:\")\n",
    "print(\"   - analysis_result.csv\")\n",
    "print(\"   - summary_stats.json\")\n",
    "print(\"   - sentiment_analysis_result.png\")\n",
    "print(\"   - wordcloud_result.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š ì¶”ê°€ í•™ìŠµ ìë£Œ\n",
    "\n",
    "### ì‹¤ì œ ë°ì´í„° ìˆ˜ì§‘ ë°©ë²•\n",
    "\n",
    "1. **ë„¤ì´ë²„ ë‰´ìŠ¤ ëŒ“ê¸€**: ë„¤ì´ë²„ API ë˜ëŠ” BeautifulSoup\n",
    "2. **ìœ íŠœë¸Œ ëŒ“ê¸€**: YouTube Data API v3\n",
    "3. **íŠ¸ìœ„í„°/X**: Twitter API v2\n",
    "\n",
    "### ì£¼ì˜ì‚¬í•­\n",
    "- ê°œì¸ì •ë³´ ë³´í˜¸ë²• ì¤€ìˆ˜\n",
    "- ì €ì‘ê¶Œ ë° ì´ìš©ì•½ê´€ í™•ì¸\n",
    "- IRB(ì—°êµ¬ìœ¤ë¦¬ìœ„ì›íšŒ) ìŠ¹ì¸ (í•™ìˆ  ì—°êµ¬ ì‹œ)\n",
    "\n",
    "### ì‹¬í™” í•™ìŠµ\n",
    "- BERT ê¸°ë°˜ í˜ì˜¤ í‘œí˜„ ë¶„ë¥˜ê¸° í•™ìŠµ\n",
    "- ì‹œê³„ì—´ ë¶„ì„ì„ í†µí•œ ê°ˆë“± í™•ì‚° íŒ¨í„´\n",
    "- ë„¤íŠ¸ì›Œí¬ ë¶„ì„ì„ í†µí•œ ì˜í–¥ë ¥ ë¶„ì„\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ í† ë¡  ì§ˆë¬¸\n",
    "\n",
    "1. ì˜¨ë¼ì¸ í˜ì˜¤ í‘œí˜„ì„ ê¸°ìˆ ì ìœ¼ë¡œ íƒì§€í•˜ëŠ” ê²ƒì˜ í•œê³„ëŠ”?\n",
    "2. ë§¥ë½ ì—†ì´ ë‹¨ì–´ë§Œìœ¼ë¡œ í˜ì˜¤ë¥¼ íŒë‹¨í•  ìˆ˜ ìˆëŠ”ê°€?\n",
    "3. í”Œë«í¼ì˜ ì•Œê³ ë¦¬ì¦˜ì´ ê°ˆë“± ì¦í­ì— ê¸°ì—¬í•˜ëŠ”ê°€?\n",
    "4. í…ìŠ¤íŠ¸ ë§ˆì´ë‹ ê²°ê³¼ë¥¼ ì •ì±…ì— ì–´ë–»ê²Œ í™œìš©í•  ìˆ˜ ìˆëŠ”ê°€?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
