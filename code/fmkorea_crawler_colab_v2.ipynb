{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ” ì—í”„ì— ì½”ë¦¬ì•„ í¬ë¡¤ëŸ¬ (Selenium - ì½”ë© ì „ìš©)\n",
        "## í˜ì´ì§€ë„¤ì´ì…˜ ë¬¸ì œ í•´ê²° ë²„ì „"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1ï¸âƒ£ í™˜ê²½ ì„¤ì • (í•„ìˆ˜! ì²˜ìŒ 1íšŒ)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ì½”ë©ìš© Selenium ì„¤ì¹˜\n",
        "!pip install selenium\n",
        "!apt-get update\n",
        "!apt install -y chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
        "\n",
        "print(\"âœ… ì„¤ì¹˜ ì™„ë£Œ!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2ï¸âƒ£ í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from datetime import datetime, timedelta\n",
        "from urllib.parse import quote, urljoin\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import random\n",
        "import time\n",
        "import re\n",
        "\n",
        "\n",
        "class FMKoreaCrawlerSelenium:\n",
        "    \"\"\"ì—í”„ì— ì½”ë¦¬ì•„ í¬ë¡¤ëŸ¬ (Selenium - ì½”ë© ì „ìš©)\"\"\"\n",
        "\n",
        "    def __init__(self, delay_min=3, delay_max=6):\n",
        "        self.base_url = \"https://www.fmkorea.com\"\n",
        "        self.delay_min = delay_min\n",
        "        self.delay_max = delay_max\n",
        "        self.driver = None\n",
        "        \n",
        "    def _init_driver(self):\n",
        "        \"\"\"ì½”ë©ìš© Chrome ë“œë¼ì´ë²„ ì´ˆê¸°í™”\"\"\"\n",
        "        print(\"ğŸ”„ ë¸Œë¼ìš°ì € ì´ˆê¸°í™” ì¤‘...\")\n",
        "        \n",
        "        chrome_options = Options()\n",
        "        \n",
        "        # ì½”ë© í•„ìˆ˜ ì˜µì…˜ë“¤\n",
        "        chrome_options.add_argument('--headless')\n",
        "        chrome_options.add_argument('--no-sandbox')\n",
        "        chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "        chrome_options.add_argument('--disable-gpu')\n",
        "        chrome_options.add_argument('--disable-software-rasterizer')\n",
        "        chrome_options.add_argument('--disable-extensions')\n",
        "        chrome_options.add_argument('--remote-debugging-port=9222')\n",
        "        chrome_options.add_argument('--window-size=1920,1080')\n",
        "        \n",
        "        # ë´‡ íƒì§€ ìš°íšŒ\n",
        "        chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
        "        chrome_options.add_experimental_option('excludeSwitches', ['enable-automation'])\n",
        "        chrome_options.add_experimental_option('useAutomationExtension', False)\n",
        "        \n",
        "        # User-Agent\n",
        "        chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')\n",
        "        \n",
        "        # ë“œë¼ì´ë²„ ìƒì„±\n",
        "        self.driver = webdriver.Chrome(options=chrome_options)\n",
        "        \n",
        "        # webdriver íƒì§€ ìš°íšŒ\n",
        "        self.driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {\n",
        "            'source': 'Object.defineProperty(navigator, \"webdriver\", {get: () => undefined})'\n",
        "        })\n",
        "        \n",
        "        print(\"âœ… ë¸Œë¼ìš°ì € ì¤€ë¹„ ì™„ë£Œ!\")\n",
        "\n",
        "    def _delay(self, extra=0):\n",
        "        time.sleep(random.uniform(self.delay_min + extra, self.delay_max + extra))\n",
        "\n",
        "    def _parse_date(self, date_str):\n",
        "        if not date_str:\n",
        "            return None\n",
        "        try:\n",
        "            today = datetime.now()\n",
        "            date_str = date_str.strip()\n",
        "            \n",
        "            match = re.search(r'(\\d{4})[.\\-/](\\d{1,2})[.\\-/](\\d{1,2})', date_str)\n",
        "            if match:\n",
        "                return datetime(int(match.group(1)), int(match.group(2)), int(match.group(3)))\n",
        "            \n",
        "            match = re.search(r'^(\\d{1,2})[.\\-/](\\d{1,2})$', date_str)\n",
        "            if match:\n",
        "                return datetime(today.year, int(match.group(1)), int(match.group(2)))\n",
        "            \n",
        "            if 'ë°©ê¸ˆ' in date_str or 'ì´ˆ ì „' in date_str:\n",
        "                return today\n",
        "            elif 'ë¶„ ì „' in date_str:\n",
        "                m = re.search(r'(\\d+)', date_str)\n",
        "                return today - timedelta(minutes=int(m.group(1))) if m else today\n",
        "            elif 'ì‹œê°„ ì „' in date_str:\n",
        "                m = re.search(r'(\\d+)', date_str)\n",
        "                return today - timedelta(hours=int(m.group(1))) if m else today\n",
        "            elif 'ì¼ ì „' in date_str:\n",
        "                m = re.search(r'(\\d+)', date_str)\n",
        "                return today - timedelta(days=int(m.group(1))) if m else today\n",
        "            return None\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "    def _scroll_down(self):\n",
        "        self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "        time.sleep(1)\n",
        "\n",
        "    def search_posts(self, keyword, max_pages=50, max_posts=200, start_date=None, end_date=None):\n",
        "        if self.driver is None:\n",
        "            self._init_driver()\n",
        "        \n",
        "        posts = []\n",
        "        visited_urls = set()\n",
        "        \n",
        "        if isinstance(start_date, str):\n",
        "            start_date = datetime.strptime(start_date, '%Y-%m-%d')\n",
        "        if isinstance(end_date, str):\n",
        "            end_date = datetime.strptime(end_date, '%Y-%m-%d')\n",
        "        if start_date is None:\n",
        "            start_date = datetime(2024, 1, 1)\n",
        "        if end_date is None:\n",
        "            end_date = datetime(2025, 12, 31)\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"ğŸ” '{keyword}' ê²€ìƒ‰ ì‹œì‘\")\n",
        "        print(f\"ğŸ“… ê¸°ê°„: {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}\")\n",
        "        print(f\"{'='*60}\\n\")\n",
        "\n",
        "        for page_num in range(1, max_pages + 1):\n",
        "            search_url = f\"{self.base_url}/search.php?search_keyword={quote(keyword)}&search_target=title_content&page={page_num}\"\n",
        "            \n",
        "            print(f\"ğŸ“„ í˜ì´ì§€ {page_num}/{max_pages}...\")\n",
        "            \n",
        "            try:\n",
        "                self.driver.get(search_url)\n",
        "                self._delay()\n",
        "                \n",
        "                WebDriverWait(self.driver, 15).until(\n",
        "                    EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
        "                )\n",
        "                \n",
        "                self._scroll_down()\n",
        "                soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"    âŒ í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
        "                continue\n",
        "            \n",
        "            found_in_page = 0\n",
        "            \n",
        "            # ê²Œì‹œë¬¼ ë§í¬ ì°¾ê¸°\n",
        "            links_found = soup.find_all('a', href=re.compile(r'/\\d+$'))\n",
        "            \n",
        "            for link in links_found:\n",
        "                if len(posts) >= max_posts:\n",
        "                    break\n",
        "                \n",
        "                href = link.get('href', '')\n",
        "                title = link.get_text(strip=True)\n",
        "                \n",
        "                if not href or not title or len(title) < 3:\n",
        "                    continue\n",
        "                \n",
        "                if not re.search(r'/\\d+$', href):\n",
        "                    continue\n",
        "                \n",
        "                full_url = urljoin(self.base_url, href)\n",
        "                \n",
        "                if full_url in visited_urls:\n",
        "                    continue\n",
        "                \n",
        "                visited_urls.add(full_url)\n",
        "                \n",
        "                parent = link.find_parent(['li', 'tr', 'div'])\n",
        "                date_str = ''\n",
        "                date_parsed = None\n",
        "                \n",
        "                if parent:\n",
        "                    date_elem = parent.find(['span', 'td'], class_=re.compile(r'date|time|regdate|side'))\n",
        "                    if date_elem:\n",
        "                        date_str = date_elem.get_text(strip=True)\n",
        "                        date_parsed = self._parse_date(date_str)\n",
        "                \n",
        "                if date_parsed and not (start_date <= date_parsed <= end_date):\n",
        "                    continue\n",
        "                \n",
        "                posts.append({\n",
        "                    'title': title,\n",
        "                    'url': full_url,\n",
        "                    'keyword': keyword,\n",
        "                    'date': date_str,\n",
        "                    'date_parsed': date_parsed\n",
        "                })\n",
        "                found_in_page += 1\n",
        "            \n",
        "            print(f\"    âœ… {found_in_page}ê°œ ë°œê²¬ (ì´ {len(posts)}ê°œ)\")\n",
        "            \n",
        "            if len(posts) >= max_posts:\n",
        "                print(f\"\\nğŸ¯ ëª©í‘œ ë‹¬ì„±!\")\n",
        "                break\n",
        "            \n",
        "            if found_in_page == 0 and page_num > 5:\n",
        "                print(f\"    âš ï¸ ë¹ˆ í˜ì´ì§€ ì—°ì†. ì¤‘ë‹¨.\")\n",
        "                break\n",
        "\n",
        "        print(f\"\\nâœ… ê²€ìƒ‰ ì™„ë£Œ! ì´ {len(posts)}ê°œ\")\n",
        "        return posts\n",
        "\n",
        "    def get_post_detail(self, url):\n",
        "        if self.driver is None:\n",
        "            self._init_driver()\n",
        "        \n",
        "        try:\n",
        "            self.driver.get(url)\n",
        "            self._delay()\n",
        "            \n",
        "            WebDriverWait(self.driver, 15).until(\n",
        "                EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
        "            )\n",
        "            \n",
        "            for _ in range(3):\n",
        "                self._scroll_down()\n",
        "            \n",
        "            soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"    âŒ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
        "            return None\n",
        "\n",
        "        result = {'url': url, 'title': '', 'author': '', 'date': '', 'content': '', 'comments': []}\n",
        "\n",
        "        # ì œëª©\n",
        "        elem = soup.select_one('span.np_18px_span') or soup.select_one('h1.title') or soup.find('title')\n",
        "        if elem:\n",
        "            result['title'] = elem.get_text(strip=True)\n",
        "\n",
        "        # ì‘ì„±ì\n",
        "        elem = soup.select_one('a.member') or soup.select_one('span.author')\n",
        "        if elem:\n",
        "            result['author'] = elem.get_text(strip=True)\n",
        "\n",
        "        # ë‚ ì§œ\n",
        "        elem = soup.select_one('span.date') or soup.select_one('span.time')\n",
        "        if elem:\n",
        "            result['date'] = elem.get_text(strip=True)\n",
        "\n",
        "        # ë³¸ë¬¸\n",
        "        elem = soup.select_one('div.xe_content') or soup.select_one('article')\n",
        "        if elem:\n",
        "            for tag in elem.find_all(['script', 'style', 'img', 'video', 'iframe']):\n",
        "                tag.decompose()\n",
        "            result['content'] = elem.get_text(separator='\\n', strip=True)\n",
        "\n",
        "        # ëŒ“ê¸€\n",
        "        for item in soup.select('div.fdb_lst_ul li, ul.fdb_lst_ul > li')[:50]:\n",
        "            content_elem = item.select_one('div.xe_content')\n",
        "            if content_elem:\n",
        "                content = content_elem.get_text(strip=True)\n",
        "                if content and len(content) > 1:\n",
        "                    author_elem = item.select_one('a.member')\n",
        "                    result['comments'].append({\n",
        "                        'author': author_elem.get_text(strip=True) if author_elem else '',\n",
        "                        'content': content\n",
        "                    })\n",
        "\n",
        "        return result\n",
        "\n",
        "    def crawl_with_details(self, keyword, max_posts=50, start_date=None, end_date=None):\n",
        "        posts = self.search_posts(keyword=keyword, max_posts=max_posts, start_date=start_date, end_date=end_date)\n",
        "        \n",
        "        if not posts:\n",
        "            print(\"âŒ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ\")\n",
        "            return [], []\n",
        "        \n",
        "        posts_data = []\n",
        "        comments_data = []\n",
        "        \n",
        "        print(f\"\\nğŸ“ ìƒì„¸ í¬ë¡¤ë§ ({len(posts)}ê°œ)\")\n",
        "        print(\"-\" * 60)\n",
        "        \n",
        "        for i, post in enumerate(posts, 1):\n",
        "            print(f\"[{i}/{len(posts)}] {post['title'][:40]}...\")\n",
        "            \n",
        "            detail = self.get_post_detail(post['url'])\n",
        "            if detail:\n",
        "                posts_data.append({\n",
        "                    'title': detail['title'] or post['title'],\n",
        "                    'url': post['url'],\n",
        "                    'author': detail['author'],\n",
        "                    'date': detail['date'],\n",
        "                    'content': detail['content'],\n",
        "                    'keyword': keyword,\n",
        "                    'comment_count': len(detail['comments'])\n",
        "                })\n",
        "                \n",
        "                for comment in detail['comments']:\n",
        "                    comments_data.append({\n",
        "                        'post_title': detail['title'] or post['title'],\n",
        "                        'post_url': post['url'],\n",
        "                        'comment_author': comment['author'],\n",
        "                        'comment_content': comment['content'],\n",
        "                        'keyword': keyword\n",
        "                    })\n",
        "                \n",
        "                print(f\"    âœ… ëŒ“ê¸€ {len(detail['comments'])}ê°œ\")\n",
        "            else:\n",
        "                print(f\"    âŒ ì‹¤íŒ¨\")\n",
        "        \n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"âœ… ì™„ë£Œ! ê²Œì‹œë¬¼: {len(posts_data)}ê°œ, ëŒ“ê¸€: {len(comments_data)}ê°œ\")\n",
        "        \n",
        "        return posts_data, comments_data\n",
        "\n",
        "    def close(self):\n",
        "        if self.driver:\n",
        "            self.driver.quit()\n",
        "            self.driver = None\n",
        "            print(\"ğŸ”’ ë¸Œë¼ìš°ì € ì¢…ë£Œ\")\n",
        "\n",
        "\n",
        "print(\"âœ… í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤ ë¡œë“œ ì™„ë£Œ!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3ï¸âƒ£ í¬ë¡¤ë§ ì‹¤í–‰"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”\n",
        "crawler = FMKoreaCrawlerSelenium(delay_min=3, delay_max=6)\n",
        "\n",
        "# ğŸ”¹ ì„¤ì •\n",
        "KEYWORD = \"ë§ˆì•½\"\n",
        "MAX_POSTS = 50\n",
        "START_DATE = \"2024-01-01\"\n",
        "END_DATE = \"2025-12-31\"\n",
        "\n",
        "# í¬ë¡¤ë§ ì‹¤í–‰\n",
        "posts_data, comments_data = crawler.crawl_with_details(\n",
        "    keyword=KEYWORD,\n",
        "    max_posts=MAX_POSTS,\n",
        "    start_date=START_DATE,\n",
        "    end_date=END_DATE\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4ï¸âƒ£ ê²°ê³¼ í™•ì¸ ë° ì €ì¥"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DataFrame ë³€í™˜\n",
        "posts_df = pd.DataFrame(posts_data)\n",
        "comments_df = pd.DataFrame(comments_data)\n",
        "\n",
        "print(f\"ğŸ“Š ê²Œì‹œë¬¼: {len(posts_df)}ê°œ\")\n",
        "print(f\"ğŸ“Š ëŒ“ê¸€: {len(comments_df)}ê°œ\")\n",
        "\n",
        "if not posts_df.empty:\n",
        "    print(\"\\nğŸ“ ê²Œì‹œë¬¼ ìƒ˜í”Œ:\")\n",
        "    display(posts_df[['title', 'date', 'comment_count']].head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ì €ì¥ ë° ë‹¤ìš´ë¡œë“œ\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "posts_file = f\"{KEYWORD}_ê²Œì‹œë¬¼_{timestamp}.xlsx\"\n",
        "comments_file = f\"{KEYWORD}_ëŒ“ê¸€_{timestamp}.xlsx\"\n",
        "\n",
        "posts_df.to_excel(posts_file, index=False)\n",
        "comments_df.to_excel(comments_file, index=False)\n",
        "\n",
        "print(f\"âœ… ì €ì¥ ì™„ë£Œ!\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download(posts_file)\n",
        "files.download(comments_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ë¸Œë¼ìš°ì € ì¢…ë£Œ\n",
        "crawler.close()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
